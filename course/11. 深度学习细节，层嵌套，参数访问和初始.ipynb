{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0526,  0.2071,  0.1756,  0.3949, -0.1839,  0.2518, -0.1591, -0.0287,\n",
       "          0.1354, -0.2595],\n",
       "        [-0.1383,  0.1457,  0.0283,  0.4037, -0.0487,  0.1715, -0.1284,  0.0828,\n",
       "         -0.0184, -0.1523]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 普通的多模组拼接，Sequential相当于一个list\n",
    "net = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "X = torch.rand(2,20) # 2批次，维度20\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2662,  0.1794,  0.1804, -0.0403, -0.1575,  0.0947,  0.0807, -0.0978,\n",
       "         -0.0828, -0.1585],\n",
       "        [-0.2209,  0.1481,  0.0659, -0.0177, -0.1130,  0.0661, -0.0120, -0.0919,\n",
       "          0.0173, -0.0893]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义MLP，自定义模型的内部细节，增加网络层等操作\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 模型内增加一层全连接层\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10) # 输出\n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0501,  0.0885,  0.1277, -0.1917, -0.0720, -0.1948, -0.0222,  0.0510,\n",
       "         -0.0804,  0.0568],\n",
       "        [ 0.0276,  0.2202,  0.1867, -0.2715, -0.1660, -0.0482,  0.0097, -0.0312,\n",
       "         -0.1331, -0.0053]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建自己的Sequential\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block]=block\n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1669, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 自定义MLP中的操作，可以生成一个矩阵但不参与训练\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=False)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "    def forward(self,X):\n",
    "        X = self.linear(X)\n",
    "        # 可以做任何操作，并且返回一个标量，这里的矩阵无意义，只是随意的操作\n",
    "        X = F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum()>1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0204, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随意嵌套层\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 定义一个组合\n",
    "        self.net = nn.Sequential(nn.Linear(20,64),nn.ReLU(),\n",
    "                                 nn.Linear(64,32),nn.ReLU())\n",
    "        # 再定义一个全连接\n",
    "        self.linear = nn.Linear(32,16)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "chimera = nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())\n",
    "chimera(X)   # 随意组合自定义嵌套层和普通链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# 参数管理\n",
    "\n",
    "模型的参数保存和访问方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3146],\n",
       "        [-0.2588]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.3289, -0.0961,  0.1278, -0.1124, -0.3463,  0.3372, -0.3152,  0.1197]])), ('bias', tensor([-0.1152]))])\n",
      "\n",
      "\n",
      "<class 'torch.nn.parameter.Parameter'> <class 'torch.nn.parameter.Parameter'>\n",
      "\n",
      "\n",
      "tensor([[ 0.3289, -0.0961,  0.1278, -0.1124, -0.3463,  0.3372, -0.3152,  0.1197]]) tensor([-0.1152])\n",
      "\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "# 访问所有参数\n",
    "print(net[2].state_dict())\n",
    "print('\\n')\n",
    "# 网络参数类型都是Parameter\n",
    "print(type(net[2].bias),type(net[2].weight))\n",
    "\n",
    "print('\\n')\n",
    "# 访问参数本身，返回的是tensor\n",
    "print(net[2].weight.data,net[2].bias.data)\n",
    "\n",
    "# 访问参数的梯度\n",
    "print('\\n')\n",
    "print(net[2].weight.grad,net[2].bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n",
      "tensor([[ 0.3289, -0.0961,  0.1278, -0.1124, -0.3463,  0.3372, -0.3152,  0.1197]])\n"
     ]
    }
   ],
   "source": [
    "# 一次性访问某个模型所有参数\n",
    "datas = [(name,param.shape) for name,param in net[0].named_parameters()]\n",
    "print(*datas)  # * 用于解包\n",
    "\n",
    "# 一次性访问某个组合模型的所有模型的所有参数  等价于 net.state_dict()\n",
    "datas = [(name,param.shape) for name,param in net.named_parameters()]\n",
    "print(*datas)\n",
    "\n",
    "# 通过列出所有模型参数，再用索引访问\n",
    "print(net.state_dict()['2.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2176],\n",
       "        [-0.2177]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 嵌套块的参数命名法则\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),\n",
    "                         nn.Linear(8,4),nn.ReLU())\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 添加模块，需要模块名和模块\n",
    "        net.add_module(f'block {i}',block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.3511, -0.1420,  0.3054, -0.0990])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看嵌套块\n",
    "print(rgnet)\n",
    "# 可以像访问list一样访问\n",
    "rgnet[0][1][2].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0024,  0.0006, -0.0174, -0.0061],\n",
       "        [ 0.0213,  0.0039,  0.0084,  0.0004],\n",
       "        [ 0.0087,  0.0153, -0.0107,  0.0029],\n",
       "        [-0.0076,  0.0026,  0.0274,  0.0053],\n",
       "        [-0.0008,  0.0248,  0.0031,  0.0070],\n",
       "        [-0.0174,  0.0061,  0.0236, -0.0080],\n",
       "        [-0.0005, -0.0086, -0.0030, -0.0001],\n",
       "        [ 0.0065,  0.0171, -0.0044, -0.0057]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数初始化\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01) # 正态分布，均值0，方差0.01\n",
    "        nn.init.zeros_(m.bias)\n",
    "# 对多层的每个模型都执行这样的初始化\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0663,  0.0467, -0.6499,  1.0888],\n",
      "        [-0.4460,  0.1132, -0.4972,  0.2118],\n",
      "        [-0.1786, -0.1659,  0.2795,  0.9941],\n",
      "        [ 0.0941,  0.0828,  0.0097, -0.4063],\n",
      "        [-0.4029, -0.3692, -0.1537,  0.0472],\n",
      "        [-0.4940, -0.4170,  0.5301,  0.3784],\n",
      "        [ 0.0917,  0.0374,  0.5043,  0.3448],\n",
      "        [-0.7164, -0.9209,  0.1347, -0.1637]])\n"
     ]
    }
   ],
   "source": [
    "# 参数初始化为常数\n",
    "def init_constant(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "# Xavier初始化\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_constant)\n",
    "print(net[0].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight torch.Size([8, 4])\n",
      "init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  8.9851,  0.0000],\n",
       "        [-7.5785, -0.0000,  9.1425, -0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义初始化\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print('init',*[(name,param.shape) for name,param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight,-10,10) # -10到10的均匀分布\n",
    "        m.weight.data *= m.weight.data.abs() >=5 # 只保留大于5的部分\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.0000,  6.0000, 14.9851,  6.0000],\n",
       "        [-1.5785,  6.0000, 15.1425,  6.0000],\n",
       "        [14.9646, -0.6480,  0.3961,  6.0000],\n",
       "        [-1.5388, 11.4065, 14.7135, 15.1823],\n",
       "        [-1.0243, 12.0140, 11.5659, 13.7363],\n",
       "        [-0.9213,  6.0000,  6.0000, 14.5679],\n",
       "        [ 6.0000,  6.0000,  6.0000,  6.0000],\n",
       "        [-0.9396, 13.7712, -3.4796,  6.0000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接手动设置参数\n",
    "net[0].weight.data[:]+=1\n",
    "net[0].weight.data[0,0]=42\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 参数绑定，即设置共享层\n",
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),\n",
    "                    shared,nn.ReLU(),\n",
    "                    shared,nn.ReLU(),\n",
    "                    nn.Linear(8,1))\n",
    "net(X)\n",
    "print(net[2].weight.data[0]==net[4].weight.data[0])\n",
    "net[2].weight.data[0,0]=100\n",
    "print(net[2].weight.data[0]==net[4].weight.data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "构建层（模型是由多个层组成，每个层执行具体的前向的矩阵乘法）\n",
    "\n",
    "层和模块都是nn.Module的子类，都自动映射了forward方法，使得调用的时候不用  xx.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module): # 定义一个层，前向函数里只将X的均值变为0\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,X):\n",
    "        return X-X.mean()\n",
    "\n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.3132e-10, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 自定义层的组合\n",
    "net = nn.Sequential(nn.Linear(8,128),CenteredLayer())\n",
    "Y = net(torch.rand(4,8))\n",
    "print(Y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1618,  0.2227,  0.1049],\n",
      "        [ 1.3816, -0.5240, -1.2592],\n",
      "        [ 0.1696, -2.5408, -0.2076],\n",
      "        [ 1.8466,  2.0874,  0.3024],\n",
      "        [-1.5016, -0.3834, -1.0029]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5458, 0.0000],\n",
       "        [1.5721, 0.0217, 0.0000]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义带参数的层\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self,in_units,units): # 指定一层的输入和输出维度\n",
    "        super().__init__()\n",
    "        # 自定义两个参数，w，b，需要指定为parameter类型，这种类型会自动保存梯度\n",
    "        self.weight = nn.Parameter(torch.randn(in_units,units))\n",
    "        # torch.randn是正态分布，torch.rand是均匀分布\n",
    "        self.bias = nn.Parameter(torch.randn(units))\n",
    "    def forward(self,X):\n",
    "        linear = torch.matmul(X,self.weight.data)+self.bias.data\n",
    "        return F.relu(linear)\n",
    "\n",
    "# 实例化一个linear\n",
    "linear = MyLinear(5,3)\n",
    "print(linear.weight.data)\n",
    "linear(torch.rand(2,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5153],\n",
       "        [1.9762],\n",
       "        [0.6330]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义层构建模型\n",
    "net = nn.Sequential(nn.Linear(64,8),MyLinear(8,1))\n",
    "net(torch.rand(3,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "# 模型保存在本地如何实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([0, 1, 2, 3]) tensor([0., 0., 0., 0.])\n",
      "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "# 1. 保存tensor张量\n",
    "x = torch.arange(4)\n",
    "torch.save(x,'./weights/x-file')  #本地文件名：x-file\n",
    "# 1.1 加载张量\n",
    "x2 = torch.load('./weights/x-file')\n",
    "print(x2)\n",
    "\n",
    "# 2. 保存和读取张量列表\n",
    "y = torch.zeros(4)\n",
    "torch.save([x,y],'./weights/x-files')\n",
    "\n",
    "x2,y2 = torch.load('./weights/x-files')\n",
    "print(x2,y2)\n",
    "\n",
    "# 3. 保存和读取张量字典\n",
    "mydict = {'x':x,'y':y}\n",
    "torch.save(mydict,'./weights/mydict')\n",
    "mydict2 = torch.load('./weights/mydict')\n",
    "print(mydict2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4940, -0.0014,  0.2467,  0.1463, -0.1547, -0.2169, -0.0428, -0.1436,\n",
      "         -0.5019, -0.2251],\n",
      "        [-0.1169,  0.0587,  0.0408,  0.2293,  0.1631,  0.0645,  0.1849, -0.1220,\n",
      "         -0.1051, -0.1488]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# 4. 模型加载和保存\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.output = nn.Linear(256,10)\n",
    "    def forward(self,x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(2,20)\n",
    "Y = net(X)\n",
    "print(Y)\n",
    "\n",
    "torch.save(net.state_dict(),'./weights/mlp.params')\n",
    "\n",
    "# 加载\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('./weights/mlp.params'))\n",
    "clone.eval()  # 切换到评估模式，用于正向 设置后不会使用dropout 评估时使用训练期间积累的全局统计数据来归一化\n",
    "Y_clone = clone(X)\n",
    "print(Y_clone==Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 24 16:17:30 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.15       Driver Version: 512.15       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0 Off |                  N/A |\n",
      "| N/A   35C    P8     3W /  N/A |      0MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
