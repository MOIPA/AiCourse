# 学习时候的一些疑惑


### 1. 写了model.eval() 为什么还要关闭梯度计算 with torch.no_grad()

某些层在训练和推理时的行为不同，model.eval()会修改这些层的内部状态：

Dropout 层：训练时随机丢弃神经元（防止过拟合），推理时保留所有神经元（输出确定值）。

BatchNorm 层：训练时使用当前批次的统计量（均值、方差），推理时使用全局统计量（已保存的 running_mean/running_var）。

RNN 层：部分 RNN 变体（如 LSTM）在训练时可能启用 dropout，推理时关闭。

总结：model.train() 和model.eval() 不影响模型变量积累梯度

### 2. metric = evaluate.load("glue", "mrpc") 怎么理解

metric = evaluate.load("glue", "mrpc") 并非加载数据集，而是加载与 GLUE 基准测试中 MRPC 任务相关的评估指标（Metric）是一组计算逻辑，用于衡量模型预测结果与真实标签的匹配程度（如计算predictions和references的准确率）。

加载 MRPC 任务的评估指标
GLUE：通用语言理解评估基准（General Language Understanding Evaluation），包含多个 NLP 任务（如 MRPC、MNLI 等）。
MRPC：微软研究释义语料库（Microsoft Research Paraphrase Corpus），任务为判断两个句子是否为释义（二分类任务）。
对应指标：MRPC 任务的官方评估指标是准确率（Accuracy）和F1 分数（F1-Score）。

### 3. range(num_training_steps) 返回的是range对象还是int整数

range(num_training_steps) 返回的是一个 range 对象，而非单个整数。

```python
num_training_steps = 5
r = range(num_training_steps)

print(type(r))        # 输出: <class 'range'>
print(list(r))        # 输出: [0, 1, 2, 3, 4]
print(r.start)        # 输出: 0
print(r.stop)         # 输出: 5
print(r.step)         # 输出: 1
```

### 4. epochs=3是遍历三遍训练集

### 5. adam和sgd区别

使用优化器来更新模型参数的时候，怎么更新是有说法的

sgd: 模型参数=当前参数- 学习率*一个batch积累的梯度

adam：根据参数的梯度历史动态调整每个参数的学习率

速度 vs 稳定性：Adam 适合快速迭代和探索，SGD 适合追求极致泛化能力。

默认选择：对于大多数深度学习任务，优先尝试 Adam 或 AdamW。

精细调优：若时间允许，SGD+Momentum 在某些任务上可能达到更好的最终性能。

### 6. PEFT LORA模型注意点

1. 调用get_peft_model（） 后，使用的基础模型结构是会被修改的，添加 LoRA 适配器层。
2. LORA微调的时候，如果指定了target_modules=["query", "key", "value", "classifier"]也就是lora作用的模块，如果不指定，LORA的trainer保存的时候保存的模块参数和作用的模块lora参数不一致，导致加载模型失败（没有合并lora和基础模型情况下分开加载）
3. LORA微调后一定要保存模型，合并保存！merged_model = model.merge_and_unload()

### 7. 字符分类任务总结


数据格式关键：id,token,ner_tags
数据还要有feature.names

分类的数据tokenize后，对应的ner_tags也要扩展

因此：也要DataCollatorForTokenClassification，能再对token扩展的同时也对labels拓展

验证，可以用evaluate的库加载seqeval

微调模型可以将模型替换为lora模型

然后开始torch手动或者trainer自动微调

### 8. 掩码类任务总结

加载数据：数据每个样本是个句子，一般的预处理方式是将所有句子连接起来然后切块，一个batch的所有句子连接起来，按照每个块128个字符切割成若干个样本

数据整理器data_collator（DataCollatorForLanguageModeling 需要mlm_probability 参数用来指定掩码比例）用来生成inputs中的掩码字符，labels则是复制原来一模一样的inputs，被遮掩的字符对应label的正确字符

不仅仅可以遮蔽单个 token，还可以一次遮蔽整个单词的所有 token，这种方法被称为全词屏蔽（whole word masking）

使用的时候正常用，在句子里添加 `[mask]`标识丢给模型即可

### 9. 计算loss的例子

logits = [2.3, -1.8]  # 积极类得分2.3，消极类得分-1.8

使用softmax 函数将 logits 转换为概率分布，y_true = [1, 0]  # 索引0表示积极，索引1表示消极

L=−(1⋅log(0.96)+0⋅log(0.04))≈0.041


### 10. 中文分词需要额外安装sentencepiece

MarianTokenizer需要安装SentencePiece库,SentencePiece 是一个开源的文本分词工具，专门用于处理多语言和无空格语言（如中文、日语）

MarianTokenizer（以及大多数基于 SentencePiece 的分词器）底层默认使用的是 BPE（Byte Pair Encoding）算法

### 11. 翻译任务总结

1. 需要下载中日的句子对，对于中文这样的无空格分词，需要下载sentencePiece库，使用其中的MarianTokenizer

2. tokenizer可以查看和指定输入和目标语言，tokenizer.source_lang,tokenizer.target_lang

3. tokenize的时候传入两个：输入预言句子和text_target目标预言句子，才能输出input_ids,attention_mask和labels，否则没有labels

4. 编写tokenize的预处理函数就要提取一个batch的两种语言的句子，分别弄成inputs和text_target

5. 如果你正在使用 T5 模型（更具体地说，一个 t5-xxx checkpoint ），模型会期望文本输入有一个前缀指示目前的任务，比如 translate: English to French

6. data_collator必须要用DataCollatorForSeq2Seq，会在padding输入inputs和attention_mask的时候也自动padding对应的labels为-100，普通的data_collator不会填充labels，初始化datacollator的时候要传入tokenizer和model

7. 评估指标使用BLEU，但是一个句子可能有多种翻译方式，所以预测结果应该是一个句子列表，而参考应该是一个句子列表的列表

8. Trainer微调的时候用，Seq2SeqTrainingArguments,Seq2SeqTrainer，可以自动调用Generate方法生成句子而不是词表概率分布序列，能增强训练

9. BLEU计算的时候采用n_gram算法，目标如果没有空格间隔会被认为是一个gram，就没法计算了，我做是中文翻译日文的任务，翻译结果和目标label下载了mecab工具来分词

### 12. Jieba中文分词库

1. jieba 是基于词典和统计的分词工具，主要用于中文分词任务

2. Hugging Face 上的中文模型确实较少直接使用 jieba 库，通常使用基于子词（Subword）的分词器，分词器由模型开发者直接集成在 Tokenizer 中，无需额外依赖 jieba

### 13. 使用Trainer

1. 如果自己写了评估函数，trainer会在训练到评估时自动传入预测值和标签值

```python
trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
def compute_metrics(eval_preds):
    preds, labels = eval_preds
```
