{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX\n",
    "\n",
    "> Open Neural Network Exchange \n",
    "\n",
    "开放的神经网络模型格式标准,就像图像中的 .jpg 或 .png，但它是为 AI 模型设计的。它可以让 AI 模型在不同框架（如 PyTorch、TensorFlow、Scikit-learn）和不同设备（如 PC、手机、边缘设备）之间互相兼容和迁移\n",
    "\n",
    "+ ONNX 模型（.onnx）:一个标准化的文件格式，描述神经网络结构和参数\n",
    "+ ONNX Runtime\t:微软开发的高性能推理引擎，支持 ONNX 模型\n",
    "+ 转换器（如 torch.onnx.export）:将 PyTorch、TensorFlow 模型转换为 ONNX 格式\n",
    "+ 兼容性:支持 PyTorch、TensorFlow、Keras、Scikit-learn、XGBoost 等\n",
    "\n",
    "## 和VLLM区别\n",
    "\n",
    "+ VLLM专注于LLM部署，而onnx是通用模型格式，不止支持LLM还支持CV等机器学习和深度学习模型\n",
    "+ VLLM 支持 LoRA，onnx不支持\n",
    "+ VLLM 常用于服务器部署，onnx更多是边缘设备\n",
    "\n",
    "## 和llama.cpp区别\n",
    "\n",
    "+ llama.cpp 只支持transformer结构，只为LLM提供部署服务和量化服务\n",
    "+ llama.cpp 可以量化到q_2，onnx量化只到int8\n",
    "+ llama.cpp 模型小，极致内存占用对边缘设备更友好\n",
    "+ llama.cpp 只支持C/C++，需要其他语言支持可以看其他项目比如llama.rn对其进行的拓展，而onnx支持多种语言包括java等\n",
    "+ llama.cpp 模型来源自hg，需要从hg的模型转换为gguf格式，而onnx支持直接从pytorch和tensorflow，TVM等导出模型\n",
    "\n",
    "## 使用\n",
    "\n",
    "ONNX支持多种生态，我的技术栈是pytorch+huggingface的Transformers库那一套，hg有专门的加速在硬件推理和训练的库：`Optimum`，支持转换为onnx模型并且运行在`onnx runtime`。\n",
    "\n",
    "1. 安装依赖库：`python -m pip install optimum`\n",
    "\n",
    "可以使用optimum库快速方便的加速训练和推理，并且将模型转为onnx格式，不过再这之前还需要指定支持onnx的运行环境，比较onnx只是种格式\n",
    "\n",
    "2. onnx运行环境，有很多我选择微软的onnxruntime：`pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]` 或者`pip install onnxruntime`\n",
    "\n",
    "** 注意：zsh环境下中括号会被转义！ install的目标要加上双引号防止转义 **\n",
    "\n",
    "3. 安装transformers和pytorch环境:`pip install transformers[torch]`\n",
    "\n",
    "4. 安装进度条增加体验：`conda install ipywidgets`\n",
    "\n",
    "### 快速体验\n",
    "\n",
    "> 如果手头没有训练项目，可以参考官方demo，相对于原版的transformers使用，需要改动的地方很小\n",
    "\n",
    "```python\n",
    "- from transformers import AutoModelForSequenceClassification\n",
    "+ from optimum.intel.openvino import OVModelForSequenceClassification\n",
    "  from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "  # Download a tokenizer and model from the Hub and convert to OpenVINO format\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "- model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "+ model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)\n",
    "\n",
    "  # Run inference!\n",
    "  classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "  results = classifier(\"He's a dreadful magician.\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9919503927230835}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原版，使用transformers库版本\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "# + from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Download a tokenizer and model from the Hub and convert to OpenVINO format\n",
    "# model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# 不知道为什么无法下载，只能手动下载到本地，本地加载了\n",
    "model_id = \"./model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "# + model = ORTModelForSequenceClassification.from_pretrained(model_id, export=True)\n",
    "\n",
    "# Run inference!\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "results = classifier(\"He's a dreadful magician.\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9919503927230835}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimum库，运行在onnxruntime上\n",
    "# 原版，使用transformers库版本\n",
    "# - from transformers import AutoModelForSequenceClassification\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Download a tokenizer and model from the Hub and convert to OpenVINO format\n",
    "# model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# 不知道为什么无法下载，只能手动下载到本地，本地加载了\n",
    "model_id = \"./model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# - model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "model = ORTModelForSequenceClassification.from_pretrained(model_id, export=True)\n",
    "\n",
    "# Run inference!\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "results = classifier(\"He's a dreadful magician.\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用onnx量化\n",
    "\n",
    "以下是一个量化的例子，并将量化后的模型保存在本地，模型大小只剩1/3了，说明量化非常省内存和空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Quantization parameters for tensor:\"/distilbert/embeddings/LayerNorm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.0/attention/Mul_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.0/attention/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.0/attention/Softmax_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.0/attention/Transpose_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.0/attention/Reshape_3_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.0/sa_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.0/ffn/activation/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.0/output_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.1/attention/Mul_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.1/attention/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.1/attention/Softmax_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.1/attention/Transpose_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.1/attention/Reshape_3_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.1/sa_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.1/ffn/activation/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.1/output_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.2/attention/Mul_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.2/attention/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.2/attention/Softmax_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.2/attention/Transpose_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.2/attention/Reshape_3_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.2/sa_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.2/ffn/activation/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.2/output_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.3/attention/Mul_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.3/attention/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.3/attention/Softmax_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.3/attention/Transpose_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.3/attention/Reshape_3_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.3/sa_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.3/ffn/activation/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.3/output_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.4/attention/Mul_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.4/attention/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.4/attention/Softmax_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.4/attention/Transpose_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.4/attention/Reshape_3_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.4/sa_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.4/ffn/activation/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.4/output_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.5/attention/Mul_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.5/attention/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.5/attention/Softmax_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.5/attention/Transpose_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.5/attention/Reshape_3_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.5/sa_layer_norm/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/distilbert/transformer/layer.5/ffn/activation/Mul_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/Gather_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/Relu_output_0\" not specified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('quanted_model')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "\n",
    "qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)\n",
    "quantizer = ORTQuantizer.from_pretrained(model)\n",
    "quantizer.quantize(save_dir=\"./quanted_model\", quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.999738872051239}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载一个量化模型\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"./quanted_model\", file_name=\"model_quantized.onnx\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./quanted_model\")\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "results = classifier(\"I love burritos!\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 使用onnx runtime训练\n",
    "\n",
    "官方提供了自己的Trainer和TrainingArguments\n",
    "\n",
    "```python\n",
    "- from transformers import Trainer, TrainingArguments\n",
    "+ from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments\n",
    "\n",
    "  # Download a pretrained model from the Hub\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "  # Define the training arguments\n",
    "- training_args = TrainingArguments(\n",
    "+ training_args = ORTTrainingArguments(\n",
    "      output_dir=\"path/to/save/folder/\",\n",
    "      optim=\"adamw_ort_fused\",\n",
    "      ...\n",
    "  )\n",
    "\n",
    "  # Create a ONNX Runtime Trainer\n",
    "- trainer = Trainer(\n",
    "+ trainer = ORTTrainer(\n",
    "      model=model,\n",
    "      args=training_args,\n",
    "      train_dataset=train_dataset,\n",
    "+     feature=\"text-classification\", # The model type to export to ONNX\n",
    "      ...\n",
    "  )\n",
    "\n",
    "  # Use ONNX Runtime for training!\n",
    "  trainer.train()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
