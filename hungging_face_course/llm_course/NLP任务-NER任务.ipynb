{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLPçš„ä¸»è¦ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token åˆ†ç±»ä»»åŠ¡\n",
    "\n",
    "å®ä½“å‘½åè¯†åˆ« ï¼ˆNERï¼‰ï¼šæ‰¾å‡ºå¥å­ä¸­çš„å®ä½“ï¼ˆå¦‚äººç‰©ã€åœ°ç‚¹æˆ–ç»„ç»‡ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªå®ä½“æŒ‡å®šä¸€ä¸ªç±»åˆ«çš„æ ‡ç­¾ï¼Œå¦‚æœæ²¡æœ‰å®ä½“åˆ™ä¼šè¾“å‡ºæ— å®ä½“çš„æ ‡ç­¾ã€‚\n",
    "\n",
    "è¯æ€§æ ‡æ³¨ ï¼ˆPOSï¼‰ï¼šå°†å¥å­ä¸­çš„æ¯ä¸ªå•è¯æ ‡è®°ä¸ºå¯¹åº”äºç‰¹å®šçš„è¯æ€§ï¼ˆå¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰ï¼‰ã€‚\n",
    "\n",
    "åˆ†å—ï¼ˆchunkingï¼‰ï¼šæ‰¾å‡ºå±äºåŒä¸€å®ä½“çš„ tokens è¿™ä¸ªä»»åŠ¡ï¼ˆå¯ä»¥ä¸è¯æ€§æ ‡æ³¨æˆ–å‘½åå®ä½“è¯†åˆ«ç»“åˆï¼‰å¯ä»¥è¢«æè¿°ä¸ºå°†ä½äºå—å¼€å¤´çš„ token èµ‹äºˆä¸€ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ B- â€ ï¼ˆBeginï¼‰ï¼Œä»£è¡¨è¯¥tokenä½äºå®ä½“çš„å¼€å¤´ï¼‰ï¼Œå°†ä½äºå—å†…çš„ tokens èµ‹äºˆå¦ä¸€ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ I- â€ï¼ˆinnerï¼‰ä»£è¡¨è¯¥tokenä½äºå®ä½“çš„å†…éƒ¨ï¼‰ï¼Œå°†ä¸å±äºä»»ä½•å—çš„ tokens èµ‹äºˆç¬¬ä¸‰ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ O â€ ï¼ˆouterï¼‰ä»£è¡¨è¯¥tokenä¸å±äºä»»ä½•å®ä½“ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CoNLL-2003 æ•°æ®é›†\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[5, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 3...</td>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 1...</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Germany, 's, representative, to, the, Europea...</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 2...</td>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 1...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                             tokens  \\\n",
       "0  0  [EU, rejects, German, call, to, boycott, Briti...   \n",
       "1  1                                 [Peter, Blackburn]   \n",
       "2  2                             [BRUSSELS, 1996-08-22]   \n",
       "3  3  [The, European, Commission, said, on, Thursday...   \n",
       "4  4  [Germany, 's, representative, to, the, Europea...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0                [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                           [22, 22]   \n",
       "2                                           [22, 11]   \n",
       "3  [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 3...   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 2...   \n",
       "\n",
       "                                          chunk_tags  \\\n",
       "0                [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                           [11, 12]   \n",
       "2                                           [11, 12]   \n",
       "3  [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 1...   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 1...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0                        [3, 0, 7, 0, 0, 0, 7, 0, 0]  \n",
       "1                                             [1, 2]  \n",
       "2                                             [5, 0]  \n",
       "3  [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, ...  \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff59a84c1a046a087ecc3475432264c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3a047097634ca7981b1407ecd0d986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5d5b41510c481982372119ae3e6a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for part,dataset in raw_datasets.items():\n",
    "    dataset.to_json(f\"./data/conll2003-{part}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU    rejects German call to boycott British lamb . \n",
      "B-ORG O       B-MISC O    O  O       B-MISC  O    O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True) # æ–‡æœ¬å·²ç»æ˜¯å•è¯åˆ—è¡¨äº†\n",
    "inputs.tokens() # tokenizerè‡ªåŠ¨æ·»åŠ äº†ä¸¤ä¸ªç¬¦å·ï¼Œå¹¶ä¸”å°†lambæ‹†åˆ†äº†ï¼Œå¯¹åº”çš„ner_tagsä¹Ÿåº”è¯¥å¤šä¸‰ä¸ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids() #å¯ä»¥çœ‹åˆ°tokenizedç»“æœçš„å•è¯å±äºåŸæ¥å•è¯çš„ä½ç½®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # æ–°å•è¯çš„å¼€å§‹!\n",
    "            current_word = word_id\n",
    "            # ç‰¹æ®Š tokens çš„æ ‡ç­¾è®¾ç½®ä¸º -100 ã€‚è¿™æ˜¯å› ä¸ºé»˜è®¤æƒ…å†µä¸‹ï¼Œ -100 ä¼šè¢«æˆ‘ä»¬çš„æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰å¿½ç•¥\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # ç‰¹æ®Šçš„token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # ä¸å‰ä¸€ä¸ª tokens ç±»å‹ç›¸åŒçš„å•è¯\n",
    "            label = labels[word_id]\n",
    "            # å¦‚æœæ ‡ç­¾æ˜¯ B-XXX æˆ‘ä»¬å°†å…¶æ›´æ”¹ä¸º I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets['train'][0]['ner_tags']\n",
    "inputs = tokenizer(raw_datasets['train'][0]['tokens'],is_split_into_words=True)\n",
    "new_labels = align_labels_with_tokens(labels,inputs.word_ids())\n",
    "print(labels)\n",
    "new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None] ['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']\n",
      "[None, 0, 1, None] ['[CLS]', 'Peter', 'Blackburn', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets['train'][:2]['tokens'],is_split_into_words=True)\n",
    "print(inputs.word_ids(0),inputs.tokens(0))\n",
    "print(inputs.word_ids(1),inputs.tokens(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†ä¸Šé¢çš„tokenizeå’Œå¯¹å…¶labelçš„æ“ä½œå°è£…å‡½æ•°\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_and_align_labels,batched=True,remove_columns=raw_datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]}, {'input_ids': [101, 1943, 14428, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1], 'labels': [-100, 1, 2, -100]}]\n",
      "{'input_ids': tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
      "           119,   102],\n",
      "        [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
      "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "# ä¸èƒ½åƒ ç¬¬ä¸‰ç«  é‚£æ ·ç›´æ¥ä½¿ç”¨ DataCollatorWithPadding å› ä¸ºé‚£æ ·åªä¼šå¡«å……è¾“å…¥ï¼ˆinputs IDã€æ³¨æ„æ©ç å’Œ tokens ç±»å‹ IDï¼‰ã€‚é™¤äº†è¾“å…¥éƒ¨åˆ†ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬è¿˜éœ€è¦å¯¹æ ‡ç­¾ä¹Ÿä½¿ç”¨ä¸è¾“å…¥å®Œå…¨ç›¸åŒçš„æ–¹å¼å¡«å……\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "sample = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "print(sample)\n",
    "batch = data_collator(sample)\n",
    "print(batch) # å¯ä»¥çœ‹åˆ°labelsä¹Ÿè¢«å¡«å……äº†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 1, 2, -100]\n",
      "tensor([-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "# å¯¹æ¯”ä¸€ä¸‹å¡«å……å‰åçš„label\n",
    "print(tokenized_datasets['train'][1]['labels'])\n",
    "print(batch['labels'][1]) # å¡«å……åçš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯ä¸ªå‘¨æœŸè®¡ç®—ä¸€æ¬¡æŒ‡æ ‡ éœ€è¦å®šä¹‰compute_metricså‡½æ•°ï¼Œè¾“å…¥ï¼šé¢„æµ‹å€¼å’Œæ ‡ç­¾æ•°ç»„ï¼Œè¿”å›å¸¦æœ‰æŒ‡æ ‡åç§°çš„è¯„ä¼°ç»“æœå­—å…¸\n",
    "# ç”¨äºè¯„ä¼° Token åˆ†ç±»é¢„æµ‹çš„ç»å…¸æ¡†æ¶æ˜¯ seqeval ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… seqeval \n",
    "# !pip install seqeval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.8888888888888888}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets['train'][0]['ner_tags']\n",
    "labels = [label_names[i] for i in labels]\n",
    "print(labels) # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„çš„label\n",
    "\n",
    "prediction = labels.copy()\n",
    "print(prediction)\n",
    "prediction[2] = 'O' # æé€ å‡ºä¸€ä¸ªé¢„æµ‹\n",
    "metric.compute(predictions=[prediction],references=[labels]) #è¾“å…¥æ˜¯é¢„æµ‹åˆ—è¡¨ï¼ˆä¸æ˜¯ä¸€ä¸ªï¼‰å’Œæ ‡ç­¾åˆ—è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€šå¸¸æ¨¡å‹é¢„æµ‹ç»“æœæ˜¯logitå’ŒçœŸå®æ ‡ç­¾ç»“æœlabelsï¼Œéœ€è¦æ‰‹åŠ¨argmax logits\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # åˆ é™¤å¿½ç•¥çš„ç´¢å¼•(ç‰¹æ®Š tokens )å¹¶è½¬æ¢ä¸ºæ ‡ç­¾\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'O', '1': 'B-PER', '2': 'I-PER', '3': 'B-ORG', '4': 'I-ORG', '5': 'B-LOC', '6': 'I-LOC', '7': 'B-MISC', '8': 'I-MISC'}\n",
      "{'O': '0', 'B-PER': '1', 'I-PER': '2', 'B-ORG': '3', 'I-ORG': '4', 'B-LOC': '5', 'I-LOC': '6', 'B-MISC': '7', 'I-MISC': '8'}\n"
     ]
    }
   ],
   "source": [
    "# tokenåˆ†ç±»é—®é¢˜ï¼Œéœ€è¦æä¾›æ ‡ç­¾è½¬æ¢å™¨\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model.config.num_labels # æŸ¥çœ‹æ¨¡å‹çš„æ ‡ç­¾æ•°é‡å¯¹ä¸å¯¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\d2lc\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# å¾®è°ƒæ¨¡å‹\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./my_model/bert-finetuned-ner',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False, # å¦‚æœç™»å½•äº†ï¼Œå¯ä»¥è®¾ç½®ä¸ºTrueä¿å­˜åˆ° hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# ä½¿ç”¨trainerå°±ä¸éœ€è¦dataloaderï¼Œåªè¦æŠŠåŸæ¥ç»™dataloaderçš„datacollatorç»™trainerå°±è¡Œäº†\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "model.num_parameters() # 110Mçš„å‚æ•°å‚ä¸è®­ç»ƒ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "# åŠ è½½æœ¬åœ°çš„å¾®è°ƒå¥½çš„æ¨¡å‹\n",
    "model = AutoModelForTokenClassification.from_pretrained('./my_model/bert-finetuned-ner/checkpoint-5268')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 50, 9])\n",
      "[0, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0]\n",
      "[CLS] EU rejects German call to boycott British lamb. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['O', 'B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# ç®€å•æµ‹è¯•ä¸€ä¸‹\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'],shuffle=False,batch_size=8,collate_fn=data_collator\n",
    ")\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "# batch = {k:v.to(torch.device('cuda')) for k,v in batch.items()}\n",
    "outputs = model(**batch)\n",
    "# print(outputs)\n",
    "print(outputs.logits.shape)\n",
    "out = torch.argmax(outputs.logits,dim=-1)\n",
    "print(out[0].tolist())\n",
    "res = [label_names[p] for p in out[0].tolist()]\n",
    "print(tokenizer.decode(batch['input_ids'][0]))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorchå¾®è°ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc983f9709fb44f4855383a0d6a9f66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.9363850555368562, 'recall': 0.9029535864978903, 'f1': 0.9193654990085922, 'accuracy': 0.9817360334373344}\n",
      "epoch 1: {'precision': 0.9434533826994278, 'recall': 0.9223428759460349, 'f1': 0.9327787021630615, 'accuracy': 0.9846353093542121}\n",
      "epoch 2: {'precision': 0.9473241332884551, 'recall': 0.9246057818659659, 'f1': 0.9358270989193683, 'accuracy': 0.9851798434096662}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# 1. è®­ç»ƒå’ŒéªŒè¯æ•°æ®\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")\n",
    "\n",
    "# 2. æ¨¡å‹\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# 3. ä¼˜åŒ–å™¨\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 4. åŠ é€Ÿå™¨é‡å®šä¹‰ æ•°æ®+æ¨¡å‹+ä¼˜åŒ–å™¨\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# 5. å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# 6. è¯„ä¼°å‡½æ•°ï¼ˆæ¨¡å‹ç»“æœæ˜¯logitsï¼‰ è¿”å›è½¬ä¸ºæ–‡å­—çš„é¢„æµ‹å’Œæ ‡ç­¾\n",
    "def postprocess(predictions, labels):  # æ¨¡å‹é¢„æµ‹ï¼ŒçœŸå®æ ‡ç­¾ï¼Œéƒ½æ˜¯æ•°å­—\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # åˆ é™¤å¿½ç•¥çš„ç´¢å¼•(ç‰¹æ®Š tokens )å¹¶è½¬æ¢ä¸ºæ ‡ç­¾\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions\n",
    "\n",
    "# 7. è®­ç»ƒå¾ªç¯\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "output_dir = './my_model/torch/bert=finetuned-ner'\n",
    "for epoch in range(num_train_epochs):\n",
    "    # è®­ç»ƒ\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # è¯„ä¼°\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # å› ä¸ºæ¨¡å‹çš„æ•°æ®åˆ†å¸ƒåœ¨ä¸åŒGPUï¼Œä¸åŒGPUä¸Šçš„batché•¿åº¦ä¸ä¸€æ ·ï¼Œè°ƒæ•´ä¸ºä¸€æ ·ï¼Œå¡«å……æ¨¡å‹çš„é¢„æµ‹å’Œæ ‡ç­¾åæ‰èƒ½è°ƒç”¨ gathere()\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions) # æ”¶é›†å„ä¸ªGPUçš„è®¡ç®—ç»“æœ\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # ä¿å­˜å¹¶ä¸Šä¼ \n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model) # æ¨¡å‹è¢«åŒ…è£…åˆ°äº†åˆ†å¸ƒå¼ç±»é‡Œï¼Œè¿™é‡Œç›´æ¥è·å–åŸå§‹æ¨¡å‹\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        # repo.push_to_hub(\n",
    "        #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        # )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT LORAæ¨¡å‹æ³¨æ„ç‚¹\n",
    "\n",
    "1. è°ƒç”¨get_peft_modelï¼ˆï¼‰ åï¼Œä½¿ç”¨çš„åŸºç¡€æ¨¡å‹ç»“æ„æ˜¯ä¼šè¢«ä¿®æ”¹çš„ï¼Œæ·»åŠ  LoRA é€‚é…å™¨å±‚ã€‚\n",
    "2. LORAå¾®è°ƒçš„æ—¶å€™ï¼Œå¦‚æœæŒ‡å®šäº†target_modules=[\"query\", \"key\", \"value\", \"classifier\"]ä¹Ÿå°±æ˜¯loraä½œç”¨çš„æ¨¡å—ï¼Œå¦‚æœä¸æŒ‡å®šï¼ŒLORAçš„trainerä¿å­˜çš„æ—¶å€™ä¿å­˜çš„æ¨¡å—å‚æ•°å’Œä½œç”¨çš„æ¨¡å—loraå‚æ•°ä¸ä¸€è‡´ï¼Œå¯¼è‡´åŠ è½½æ¨¡å‹å¤±è´¥ï¼ˆæ²¡æœ‰åˆå¹¶loraå’ŒåŸºç¡€æ¨¡å‹æƒ…å†µä¸‹åˆ†å¼€åŠ è½½ï¼‰\n",
    "3. LORAå¾®è°ƒåä¸€å®šè¦ä¿å­˜æ¨¡å‹ï¼Œåˆå¹¶ä¿å­˜ï¼merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœç”¨loraæ¥åšå¾®è°ƒä¼šå¿«å¾ˆå¤š\n",
    "from transformers import AutoModelForTokenClassification\n",
    "# åŠ è½½æœ¬åœ°çš„å¾®è°ƒå¥½çš„æ¨¡å‹ åŠ è½½æœ¬åœ°æ¨¡å‹ä¹‹å‰ï¼Œä¸€å®šè¦\n",
    "model = AutoModelForTokenClassification.from_pretrained('./my_model/bert-finetuned-ner/checkpoint-5268')\n",
    "from peft import LoraConfig,get_peft_model,TaskType\n",
    "config = LoraConfig(TaskType.TOKEN_CLS,lora_alpha=32,lora_dropout=0.1,r=8,)\n",
    "model = get_peft_model(model=model,peft_config=config)\n",
    "\n",
    "from transformers import Trainer,TrainingArguments\n",
    "\n",
    "# model.to(torch.device('cuda'))\n",
    "args = TrainingArguments(\n",
    "    output_dir='./my_model/lora/bert-finetuned-ner',\n",
    "    learning_rate=5e-5,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.1,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 301,833 || all params: 108,028,434 || trainable%: 0.2794\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters() # è®­ç»ƒå‚æ•°é™ä½åˆ°äº†0.3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.8333333333333334, 'recall': 0.8490566037735849, 'f1': 0.8411214953271028, 'accuracy': 0.9620253164556962}\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ä¸€ä¸‹æ•ˆæœ\n",
    "# loraæ¨¡å‹å¾®è°ƒçš„æ•ˆæœ\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "valid_loader = DataLoader(tokenized_datasets['validation'],batch_size=32,shuffle=True,collate_fn=data_collator)\n",
    "model.to(torch.device('cpu'))\n",
    "model.eval()\n",
    "for batch in valid_loader:\n",
    "    with torch.no_grad():\n",
    "        batch = {k:v.to(torch.device('cpu'))for k,v in batch.items()}\n",
    "        print(compute_metrics((model(**batch).logits,batch['labels'])))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å­—ç¬¦åˆ†ç±»ä»»åŠ¡æ€»ç»“\n",
    "\n",
    "\n",
    "æ•°æ®æ ¼å¼å…³é”®ï¼šid,token,ner_tags\n",
    "æ•°æ®è¿˜è¦æœ‰feature.names\n",
    "\n",
    "åˆ†ç±»çš„æ•°æ®tokenizeåï¼Œå¯¹åº”çš„ner_tagsä¹Ÿè¦æ‰©å±•\n",
    "\n",
    "å› æ­¤ï¼šä¹Ÿè¦DataCollatorForTokenClassificationï¼Œèƒ½å†å¯¹tokenæ‰©å±•çš„åŒæ—¶ä¹Ÿå¯¹labelsæ‹“å±•\n",
    "\n",
    "éªŒè¯ï¼Œå¯ä»¥ç”¨evaluateçš„åº“åŠ è½½seqeval\n",
    "\n",
    "å¾®è°ƒæ¨¡å‹å¯ä»¥å°†æ¨¡å‹æ›¿æ¢ä¸ºloraæ¨¡å‹\n",
    "\n",
    "ç„¶åå¼€å§‹torchæ‰‹åŠ¨æˆ–è€…trainerè‡ªåŠ¨å¾®è°ƒ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
