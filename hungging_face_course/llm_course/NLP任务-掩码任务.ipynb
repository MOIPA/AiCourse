{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27caa6f2",
   "metadata": {},
   "source": [
    "## æ©ç ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ee014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8a6797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> DistilBERT number of parameters: 67M'\n",
      "'>>> BERT number of parameters: 110M'\n"
     ]
    }
   ],
   "source": [
    "distilbert_num_parameters = model.num_parameters()/1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349b63a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2840f24b2894686b0cf1205e44627f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\d2lc\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tassa\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7554b11ac449eba614e75c891ac4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496cf03e181648149e1beca3e4c28f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"This is a great [MASK].\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cfa9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30522])\n",
      "[3066, 3112, 6172, 2801, 8658]\n",
      "'>>> This is a great deal.'\n",
      "'>>> This is a great success.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great feat.'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "inputs = tokenizer(text,return_tensors='pt')\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_index = torch.where(inputs['input_ids']==tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0,mask_token_index]\n",
    "top_5_tokens = torch.topk(mask_token_logits,5,dim=1).indices[0].tolist()\n",
    "print(mask_token_logits.shape) # å€™é€‰å•è¯çš„logitsæ’åˆ—ï¼Œåºå·ä»£è¡¨å•è¯åºå·  topkæ˜¯æ’åºé€‰æ‹©å€¼æœ€å¤§çš„ indeicesæ˜¯é€‰æ‹©ä¸‹æ ‡\n",
    "print(top_5_tokens)\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34145a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9f0896dfaf49be8686153ddd0b07e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1edc27d07d14f3a9bcb9ab063afa286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dd28667030402f93d9e5a8f69a4e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49b9c766148494d94069351af0395e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9196213a7645068cd6adb50dfd6f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5472eefdb8a4a4c854ee5950819cce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¸‹è½½IMDBç”µå½±è¯„è®ºæ•°æ®é›†\n",
    "from datasets import load_dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d688a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...'\n",
      "'>>> Label: 1'\n",
      "\n",
      "'>>> Review: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.'\n",
      "'>>> Label: 1'\n",
      "\n",
      "'>>> Review: George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.'\n",
      "'>>> Label: 0'\n"
     ]
    }
   ],
   "source": [
    "sample = imdb_dataset['train'].shuffle(seed=42).select(range(3))\n",
    "# 0æ˜¯è´Ÿé¢è¯„è®ºï¼Œ1æ˜¯æ­£é¢\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
    "    print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63911bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['neg', 'pos'], id=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['train'].features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4b476f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679292f29c6148a9a8af01b777d34923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303c4107beee48e78795adfc7d1607a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d352312834b41ff9fc185393aa75597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "        # word_ids(i) ç”¨äºè·å–ç¬¬ i ä¸ªæ ·æœ¬ä¸­æ¯ä¸ª token å¯¹åº”çš„åŸå§‹æ–‡æœ¬ä¸­çš„ å•è¯ç´¢å¼•\n",
    "    return result\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ batched=True æ¥æ¿€æ´»å¿«é€Ÿå¤šçº¿ç¨‹!\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd342f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Review 0 length: 363'\n",
      "'>>> Review 1 length: 304'\n",
      "'>>> Review 2 length: 133'\n",
      "'>>> Concatenated reviews length: 800'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 32'\n"
     ]
    }
   ],
   "source": [
    "# ç°åœ¨æˆ‘ä»¬å·²ç»å¯¹ç”µå½±è¯„è®ºè¿›è¡Œäº† tokenizeï¼Œä¸‹ä¸€æ­¥æ˜¯å°†å®ƒä»¬å…¨éƒ¨ç»„åˆåœ¨ä¸€èµ·å¹¶å°†ç»“æœåˆ†å‰²æˆå—\n",
    "tokenizer.model_max_length # æœ€å¤§æ”¯æŒçš„å—å¤§å°\n",
    "chunk_size = 128\n",
    "# åˆ‡ç‰‡ä¼šä¸ºæ¯ä¸ªç‰¹å¾ç”Ÿæˆä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")\n",
    "\n",
    "#ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç®€å•çš„å­—å…¸æ¨å¯¼å¼å°†æ‰€æœ‰è¿™äº›ç¤ºä¾‹è¿æ¥åœ¨ä¸€èµ·ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n",
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")\n",
    "\n",
    "# è¿æ¥çš„è¯„è®ºæ‹†åˆ†ä¸ºå¤§å°ä¸º chunk_size çš„å—\n",
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)] \n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")\n",
    "# å¦‚æœæœ€åä¸€ä¸ªå—å°äº chunk_size ï¼Œå°±ä¸¢å¼ƒ æœ€åè®©æˆ‘ä»¬å°†ä¸Šè¿°æ‰€æœ‰é€»è¾‘åŒ…è£…åœ¨ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°†å…¶åº”ç”¨äºæˆ‘ä»¬çš„å·²åˆ†è¯æ•°æ®é›†ä¸Š\n",
    "\n",
    "def group_texts(examples):\n",
    "    # æ‹¼æ¥æ‰€æœ‰çš„æ–‡æœ¬\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # è®¡ç®—æ‹¼æ¥æ–‡æœ¬çš„é•¿åº¦\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # å¦‚æœæœ€åä¸€ä¸ªå—å°äº chunk_size,æˆ‘ä»¬å°†å…¶ä¸¢å¼ƒ\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # æŒ‰æœ€å¤§é•¿åº¦åˆ†å—\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # åˆ›å»ºä¸€ä¸ªæ–°çš„ labels åˆ—\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    #åœ¨ group_texts() çš„æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ labels åˆ—ï¼Œå®ƒæ˜¯é€šè¿‡å¤åˆ¶ input_ids åˆ—å½¢æˆçš„ã€‚è¿™æ˜¯å› ä¸ºåœ¨æ©ç è¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ˜¯é¢„æµ‹è¾“å…¥ä¸­éšæœºé®ä½(Masked)çš„ tokenï¼Œæˆ‘ä»¬ä¿å­˜äº†è®©æˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹ä»ä¸­å­¦ä¹  [Mask] çš„ç­”æ¡ˆ\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cadc1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232721b1239540ecba49f08f897f1673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0729b624d2f4463aa53444480d35db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f34c1d4fdf441ae92f9cd5127f718fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61291\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59904\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 122957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts,batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a1459aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it ' s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 118, 119, 120, 121, 122, 123, 124]\n",
      "[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(lm_datasets['train']['input_ids'][1]))\n",
    "print(lm_datasets['train']['word_ids'][0])\n",
    "print(lm_datasets['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b602a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids å’Œ labelä¸€æ‘¸ä¸€æ ·ï¼Œåœ¨è¾“å…¥ä¸­éšæœºæ’å…¥ [MASK] token\n",
    "# æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç‰¹æ®Šçš„æ•°æ®æ•´ç†å™¨ï¼Œå®ƒå¯ä»¥éšæœºå±è”½æ¯æ‰¹æ–‡æœ¬ä¸­çš„ä¸€äº› tokensã€‚å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤— Transformers ä¸ºè¿™é¡¹ä»»åŠ¡å‡†å¤‡äº†ä¸“ç”¨çš„ DataCollatorForLanguageModeling ã€‚æˆ‘ä»¬åªéœ€è¦å°† tokenizer å’Œä¸€ä¸ª mlm_probability å‚æ•°ï¼ˆæ©ç›– tokens çš„æ¯”ä¾‹ï¼‰ä¼ é€’ç»™å®ƒã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å°† mlm_probability å‚æ•°è®¾ç½®ä¸º 15%ï¼Œè¿™æ˜¯ BERT é»˜è®¤çš„æ•°é‡ï¼Œä¹Ÿæ˜¯æ–‡çŒ®ä¸­æœ€å¸¸è§çš„é€‰æ‹©\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da876862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'word_ids', 'labels'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "samples[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1476e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] [MASK] rented i am curious - yellow from [MASK] video store because of all the controversy that surrounded it when it was first released in [MASK]. i also heard [MASK] at first it was seized by u. s. customs aviation it [MASK] tried to [MASK] this country, therefore being a fan of films considered \" controversial \" researcher really had to see this [MASK] myself. < br / > < br [MASK] > the plot is centered around a young swedish [MASK] student named [MASK] who wants [MASK] learn everything [MASK] can about life. in particular ã‚“ [MASK] [MASK] focus her [MASK]s [MASK] making some [MASK] of documentary on what the average swede [MASK] about [MASK] political issues such'\n",
      "\n",
      "'>>> as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men [MASK] < br / > < br / > what kills me about i am curious - pedro is that 40 [MASK] [MASK], this was considered pornographic. really, the sex and nudity scenes are few and [MASK] between, even then [MASK] ' s not shot like some cheaply made porno. while my countrymen mind find [MASK] shocking, in reality sex and nudity are a [MASK] staple [MASK] swedish cinema. even ing [MASK] bergman,'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\") # åˆ é™¤è¿™ä¸ªå­—æ®µ\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bb9f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # åˆ›å»ºä¸€ä¸ªå•è¯ä¸å¯¹åº” token ç´¢å¼•ä¹‹é—´çš„æ˜ å°„\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # éšæœºé®è”½å•è¯\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d8d443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] [MASK] [MASK] i am curious - yellow [MASK] my [MASK] store because of all [MASK] controversy that surrounded it when it [MASK] first released in 1967. [MASK] also heard [MASK] [MASK] first [MASK] was seized by u [MASK] s. customs if it ever tried to enter this country, therefore [MASK] a fan [MASK] films considered \" controversial [MASK] i [MASK] had [MASK] see this for myself. < [MASK] / [MASK] < [MASK] / [MASK] the [MASK] is [MASK] around a young swedish drama student named lena who wants to [MASK] everything [MASK] can about life. in particular she [MASK] to focus [MASK] attentions to making some sort of documentary on what the average swede thought about certain political issues such'\n",
      "\n",
      "'>>> as the [MASK] [MASK] and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions [MASK] politics, she [MASK] sex with her [MASK] teacher, [MASK], and [MASK] men [MASK] < [MASK] [MASK] > < br / [MASK] what kills me about [MASK] am curious - yellow is that 40 years ago, this was [MASK] pornographic. really, the sex and nudity scenes are few and far [MASK], even [MASK] it ' s [MASK] shot like some cheaply [MASK] porno. [MASK] my countrymen mind find it [MASK], [MASK] [MASK] sex and nudity are a major [MASK] in swedish cinema. even [MASK] [MASK] bergman,'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d11b2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç®€åŒ–æ•°æ®è§„æ¨¡\n",
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207d184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tassa\\AppData\\Local\\Temp\\ipykernel_34104\\2232700579.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "\n",
    "batch_size = 64\n",
    "# åœ¨æ¯ä¸ª epoch è¾“å‡ºè®­ç»ƒçš„ loss\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./my_model/{model_name}-fintuned-imdb',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy='epoch',  # æ¯ä¸ªepochæ‰§è¡Œevaluateï¼Œåœ¨éªŒè¯é›†è®°å½•å‡†ç¡®ç‡ç­‰\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,                  # fp16 æŒ‡çš„æ˜¯ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ï¼ˆ16 ä½æµ®ç‚¹æ•°ï¼ŒFloat16ï¼‰è¿›è¡Œè®¡ç®—ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„å•ç²¾åº¦æµ®ç‚¹æ•°ï¼ˆ32 ä½æµ®ç‚¹æ•°ï¼ŒFloat32ï¼‰ã€‚è¿™æ˜¯ä¸€ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œä¸»è¦ç”¨äºåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹å¹¶å‡å°‘æ˜¾å­˜å ç”¨\n",
    "    logging_steps=logging_steps, #è¡¨ç¤ºæ¯è®­ç»ƒ logging_steps æ­¥å°±è®°å½•ä¸€æ¬¡è®­ç»ƒç›¸å…³çš„æ—¥å¿—ä¿¡æ¯ï¼Œå¦‚æŸå¤±å€¼ã€å‡†ç¡®ç‡ç­‰\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=downsampled_dataset['train'],\n",
    "    eval_dataset=downsampled_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10898235",
   "metadata": {},
   "source": [
    "### å›°æƒ‘åº¦\n",
    "\n",
    "è®­ç»ƒä¹‹å‰å·¥ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32c9af6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa8a99b88e349ccaea11e76bc86a69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity:21.94\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity:{math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "178512c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52dc67a70e34618a6cd5756e329734b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6806, 'grad_norm': 3.7878775596618652, 'learning_rate': 1.3460721868365181e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b331d38ae0fd4b4fa9be633d0d8715e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.511887311935425, 'eval_model_preparation_time': 0.0011, 'eval_runtime': 1.425, 'eval_samples_per_second': 701.737, 'eval_steps_per_second': 11.228, 'epoch': 1.0}\n",
      "{'loss': 2.5893, 'grad_norm': 3.616826295852661, 'learning_rate': 6.836518046709129e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44016c9c2ac4c428ebf8c3847958e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4500088691711426, 'eval_model_preparation_time': 0.0011, 'eval_runtime': 1.4313, 'eval_samples_per_second': 698.644, 'eval_steps_per_second': 11.178, 'epoch': 2.0}\n",
      "{'loss': 2.5282, 'grad_norm': 3.6923487186431885, 'learning_rate': 2.1231422505307855e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73d5856edec4191b892eb2a34c92779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.482969045639038, 'eval_model_preparation_time': 0.0011, 'eval_runtime': 1.4482, 'eval_samples_per_second': 690.517, 'eval_steps_per_second': 11.048, 'epoch': 3.0}\n",
      "{'train_runtime': 116.2255, 'train_samples_per_second': 258.119, 'train_steps_per_second': 4.052, 'train_loss': 2.5988421561611688, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=471, training_loss=2.5988421561611688, metrics={'train_runtime': 116.2255, 'train_samples_per_second': 258.119, 'train_steps_per_second': 4.052, 'total_flos': 994208670720000.0, 'train_loss': 2.5988421561611688, 'epoch': 3.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è®­ç»ƒ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7d89991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a8945a866e4249be958419b43cb375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity:12.06\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity:{math.exp(eval_results['eval_loss']):.2f}\")\n",
    "# å¯ä»¥çœ‹åˆ°å›°æƒ‘åº¦æ˜¯é™ä½çš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3daaef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bac85c3c6e34172ae852f43cc81141f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataCollatorForLanguageModeling åœ¨æ¯æ¬¡è¯„ä¼°æ—¶ä¹Ÿä¼šè¿›è¡Œéšæœºé®ç½©ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ¯æ¬¡è®­ç»ƒè¿è¡Œä¸­éƒ½ä¼šçœ‹åˆ°å›°æƒ‘åº¦å¾—åˆ†æœ‰äº›æ³¢åŠ¨ã€‚æ¶ˆé™¤è¿™ç§éšæœºæ€§çš„ä¸€ç§æ–¹æ³•æ˜¯åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Š ä»…è¿›è¡Œä¸€æ¬¡ é®ç½©\n",
    "\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # ä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸€åˆ—åˆ›å»ºä¸€ä¸ªæ–°çš„\"masked\"åˆ—\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68ce181",
   "metadata": {},
   "source": [
    "### pytorchæ‰‹åŠ¨è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32dd6fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa79e5260fe4e64ae588dcfa6ffc54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 12.085213716475206\n",
      ">>> Epoch 1: Perplexity: 11.544630048050726\n",
      ">>> Epoch 2: Perplexity: 11.379201422045155\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator  \n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "batch_size = 64\n",
    "train_data_loader = DataLoader(\n",
    "    downsampled_dataset['train'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_data_loader =  DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "optimizer = AdamW(model.parameters(),lr=5e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model,optimizer,train_dataloader,eval_dataloader = accelerator.prepare(model,optimizer,train_data_loader,eval_data_loader)\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0, # é¢„çƒ­é˜¶æ®µä¼šåœ¨è®­ç»ƒå¼€å§‹æ—¶ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œé€æ­¥å¢åŠ åˆ°æ­£å¸¸å­¦ä¹ ç‡ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°æ”¶æ•›\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "output_dir=f'./my_model/{model_name}-fintuned-imdb-accelerator'\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # è®­ç»ƒ\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # è¯„ä¼°\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # ä¿å­˜å¹¶ä¸Šä¼ \n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e713ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_fill = pipeline('fill-mask','./my_model/distilbert-base-uncased-fintuned-imdb-accelerator')\n",
    "pred=mask_fill(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27c0b8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a great film.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]['sequence']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
