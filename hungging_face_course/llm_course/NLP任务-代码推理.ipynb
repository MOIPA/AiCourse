{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96318295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–æ€§èƒ½\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# æ¨¡å‹å’Œæ•°æ®é›†é…ç½®\n",
    "MODEL_ID = \"Qwen/Qwen2.5-7B\"\n",
    "DATASET_PATH = \"EvolInstruct-Code-80k\"  # æœ¬åœ°æ•°æ®é›†è·¯å¾„\n",
    "MAX_LENGTH = 2048  # åºåˆ—æœ€å¤§é•¿åº¦\n",
    "OUTPUT_DIR = \"./my_model/qwen2.5-7b-code-finetuned\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e0e379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since nickrosh/Evol-Instruct-Code-80k-v1 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\tassa\\.cache\\huggingface\\datasets\\nickrosh___evol-instruct-code-80k-v1\\default\\0.0.0\\3ae930c20d5496e2c8386872d5628c45f6957db4 (last modified on Tue May 27 21:54:19 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 78264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('nickrosh/Evol-Instruct-Code-80k-v1')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568e03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½tokenizerå’Œæ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Qwenæ¨¡å‹pad_tokenä¸eos_tokenç›¸åŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecfe0356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['instruction', 'output'],\n",
       "     num_rows: 62600\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['instruction', 'output'],\n",
       "     num_rows: 7825\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['instruction', 'output'],\n",
       "     num_rows: 7826\n",
       " })}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†\n",
    "def load_and_process_data():\n",
    "    # åŠ è½½æ•°æ®é›†\n",
    "    dataset = ds\n",
    "    \n",
    "    # è¿‡æ»¤è¿‡é•¿æ ·æœ¬ï¼ˆä¿ç•™<2048 tokensï¼‰\n",
    "    def filter_long_samples(example):\n",
    "        tokens = tokenizer(example[\"instruction\"] + example[\"output\"], truncation=False)\n",
    "        return len(tokens[\"input_ids\"]) <= MAX_LENGTH\n",
    "    \n",
    "    filtered_dataset = dataset.filter(filter_long_samples)\n",
    "    \n",
    "    # åˆ’åˆ†æ•°æ®é›†ï¼ˆè®­ç»ƒ:éªŒè¯:æµ‹è¯• = 8:1:1ï¼‰\n",
    "    train_val_dataset = filtered_dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "    val_test_dataset = train_val_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "    \n",
    "    return {\n",
    "        \"train\": train_val_dataset[\"train\"],\n",
    "        \"validation\": val_test_dataset[\"train\"],\n",
    "        \"test\": val_test_dataset[\"test\"],\n",
    "    }\n",
    "\n",
    "datasets = load_and_process_data()\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ee4b9",
   "metadata": {},
   "source": [
    "# baselineè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f90ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70f87c3863c4a428ee4386c1d589184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½tokenizerå’Œæ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True,padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Qwenæ¨¡å‹pad_tokenä¸eos_tokenç›¸åŒ\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,  # ä½¿ç”¨BF16è¿›è¡Œè¯„ä¼°ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5809f87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90a1c2d9d144678b3b169dcc00825ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# æ ¼å¼åŒ–æ•°æ®é›†\n",
    "def format_dataset(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    # Qwenæ¨¡å‹çš„æŒ‡ä»¤æ ¼å¼ï¼ˆåªåŒ…å«æŒ‡ä»¤éƒ¨åˆ†ï¼Œè®©æ¨¡å‹ç”Ÿæˆå›å¤ï¼‰\n",
    "    formatted_text = f\"[INST] {instruction} [/INST]\"\n",
    "    return {\"text\": formatted_text, \"reference\": example[\"output\"]}\n",
    "\n",
    "# åŠ è½½å¹¶å¤„ç†éªŒè¯æ•°æ®\n",
    "validation_dataset = datasets['test']\n",
    "validation_dataset = validation_dataset.map(format_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cfa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_valid_set = validation_dataset.shuffle().select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5448880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ç”Ÿæˆé…ç½®\n",
    "generation_config = GenerationConfig(\n",
    "    # temperature=0.1,  # ä½æ¸©åº¦ï¼Œæé«˜ç¡®å®šæ€§\n",
    "    max_new_tokens=1024,  # æœ€å¤§ç”Ÿæˆé•¿åº¦\n",
    "    # top_p=0.9,\n",
    "    # top_k=40,\n",
    "    num_beams=1,  # ä½¿ç”¨è´ªå©ªè§£ç ï¼ˆæ›´å¿«ï¼‰\n",
    "    # early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–BLEUè¯„ä¼°å™¨\n",
    "bleu = load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72019bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°åŸºçº¿BLEUåˆ†æ•°...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [1:41:52<00:00, 244.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "åŸºçº¿BLEUåˆ†æ•°:\n",
      "bleu-1: 0.5731\n",
      "bleu-2: 0.4842\n",
      "bleu-3: 0.4253\n",
      "bleu-4: 0.3817\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './my_model/qwen2.5-7b-code-finetuned\\\\baseline_bleu_scores.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# ä¿å­˜ç»“æœ\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaseline_bleu_scores.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m bleu_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     60\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2lc\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './my_model/qwen2.5-7b-code-finetuned\\\\baseline_bleu_scores.txt'"
     ]
    }
   ],
   "source": [
    "# æ‰¹é‡è¯„ä¼°å‡½æ•°\n",
    "def evaluate_bleu(model, dataset, batch_size=4):\n",
    "    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    # åˆ†æ‰¹å¤„ç†æ•°æ®\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        \n",
    "        # å‡†å¤‡è¾“å…¥\n",
    "        inputs = tokenizer(\n",
    "            batch[\"text\"], \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=MAX_LENGTH\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # ç”Ÿæˆå›å¤\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "        \n",
    "        # è§£ç ç”Ÿæˆçš„å›å¤\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.input_ids.shape[1]:],  # åªå–ç”Ÿæˆçš„éƒ¨åˆ†\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # æ”¶é›†é¢„æµ‹å’Œå‚è€ƒ\n",
    "        all_predictions.extend(generated_texts)\n",
    "        all_references.extend([[ref] for ref in batch[\"reference\"]])  # BLEUéœ€è¦åˆ—è¡¨çš„åˆ—è¡¨\n",
    "    \n",
    "    # è®¡ç®—BLEUåˆ†æ•°ï¼ˆåŒ…æ‹¬BLEU-1åˆ°BLEU-4ï¼‰\n",
    "    results = {}\n",
    "    for n in range(1, 5):\n",
    "        results[f\"bleu-{n}\"] = bleu.compute(\n",
    "            predictions=all_predictions,\n",
    "            references=all_references,\n",
    "            max_order=n\n",
    "        )[\"bleu\"]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# æ‰§è¡Œè¯„ä¼°\n",
    "print(\"å¼€å§‹åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°åŸºçº¿BLEUåˆ†æ•°...\")\n",
    "bleu_results = evaluate_bleu(model, sample_valid_set)\n",
    "\n",
    "# æ‰“å°ç»“æœ\n",
    "print(\"\\nåŸºçº¿BLEUåˆ†æ•°:\")\n",
    "for key, value in bleu_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nåŸºçº¿åˆ†æ•°å·²ä¿å­˜è‡³: {os.path.join(OUTPUT_DIR, 'baseline_bleu_scores.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dc2ef",
   "metadata": {},
   "source": [
    "# å¾®è°ƒæ¨¡å‹\n",
    "\n",
    "4ä½é‡åŒ–æˆ–è€…8ä½é‡åŒ–å°±æ˜¯QLoraå½¢å¼ï¼Œå‡å°‘å†…å­˜ï¼Œä½†æ˜¯ä¼šå¢å¤§è®­ç»ƒæ—¶é—´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10253640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# æ ¼å¼åŒ–æ•°æ®é›†ï¼ˆæ·»åŠ ç‰¹æ®Štokenï¼‰\n",
    "def format_dataset(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    response = example[\"output\"]\n",
    "    # Qwenæ¨¡å‹çš„æŒ‡ä»¤æ ¼å¼\n",
    "    formatted_text = f\"[INST] {instruction} [/INST] {response}\"\n",
    "    return {\"text\": formatted_text}\n",
    "# ç¼–ç æ•°æ®é›†\n",
    "def encode_dataset(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n",
    "\n",
    "# æ ¼å¼åŒ–å’Œç¼–ç \n",
    "for split in datasets:\n",
    "    datasets[split] = datasets[split].map(format_dataset)\n",
    "    datasets[split] = datasets[split].map(encode_dataset, batched=True)\n",
    "    datasets[split] = datasets[split].remove_columns([\"instruction\", \"output\", \"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3aad15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e0c648b76f4a849ce82d2a3ed41466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# !pip install bitsandbytes-cuda12x\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# é‡åŒ–é…ç½®ï¼ˆINT4ï¼‰\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "# )\n",
    "\n",
    "# LoRAé…ç½®\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# æ•°æ®æ”¶é›†å™¨\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# æŒ‰ç…§é‡åŒ–é…ç½®åŠ è½½æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    # quantization_config=bnb_config, # QLORAä½¿ç”¨\n",
    "    torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,  # ä½¿ç”¨BF16æˆ–FP16\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False\n",
    ")\n",
    "model.enable_input_require_grads()\n",
    "# å‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥æ¨¡å‹çš„æ¢¯åº¦çŠ¶æ€\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter {name} has requires_grad=True\")\n",
    "    else:\n",
    "        print(f\"Parameter {name} has requires_grad=False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\d2lc\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0c3b76cfd64f9cb9b4a72afeb0b7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sets = datasets['train'].shuffle().select(range(1000))\n",
    "valid_sets = datasets['validation'].shuffle().select(range(100))\n",
    "\n",
    "# !pip install tensorboard\n",
    "model.train()\n",
    "# è®­ç»ƒå‚æ•°é…ç½®\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  # ç­‰æ•ˆbatch_size=16\n",
    "    # fp16=True,\n",
    "    bf16=True,  # ä½¿ç”¨BF16è¿›è¡Œè®­ç»ƒ\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    dataloader_num_workers=4,  # å¹¶è¡Œæ•°æ®åŠ è½½\n",
    "    gradient_checkpointing=True,  # èŠ‚çœæ˜¾å­˜\n",
    "    remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰åˆ—ç”¨äºè¯„ä¼°\n",
    ")\n",
    "\n",
    "\n",
    "# BLEUè¯„ä¼°å‡½æ•°\n",
    "def compute_metrics(eval_preds):\n",
    "    bleu = load(\"bleu\")\n",
    "    predictions, labels = eval_preds\n",
    "    # å°†é¢„æµ‹å’Œæ ‡ç­¾è½¬æ¢ä¸ºæ–‡æœ¬\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # æ ‡ç­¾ä¸­çš„-100éœ€è¦æ›¿æ¢ä¸ºpadding token id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # è®¡ç®—BLEU-4\n",
    "    results = bleu.compute(predictions=decoded_preds, references=decoded_labels, max_order=4)\n",
    "    return {\"bleu-4\": results[\"bleu\"]}\n",
    "\n",
    "# åˆ›å»ºTrainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sets,\n",
    "    eval_dataset=valid_sets,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ä¿å­˜LoRAæƒé‡ï¼ˆä»…éœ€çº¦60-100MBï¼‰\n",
    "model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_lora_weights\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "# test_results = trainer.evaluate(datasets[\"test\"])\n",
    "test_results = trainer.evaluate(sample_valid_set)\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
