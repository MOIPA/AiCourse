{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96318295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "# 设置环境变量优化性能\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# 模型和数据集配置\n",
    "MODEL_ID = \"Qwen/Qwen2.5-7B\"\n",
    "DATASET_PATH = \"EvolInstruct-Code-80k\"  # 本地数据集路径\n",
    "MAX_LENGTH = 2048  # 序列最大长度\n",
    "OUTPUT_DIR = \"./my_model/qwen2.5-7b-code-finetuned\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e0e379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since nickrosh/Evol-Instruct-Code-80k-v1 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\tassa\\.cache\\huggingface\\datasets\\nickrosh___evol-instruct-code-80k-v1\\default\\0.0.0\\3ae930c20d5496e2c8386872d5628c45f6957db4 (last modified on Tue May 27 21:54:19 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 78264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('nickrosh/Evol-Instruct-Code-80k-v1')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568e03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载tokenizer和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Qwen模型pad_token与eos_token相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecfe0356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['instruction', 'output'],\n",
       "     num_rows: 62600\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['instruction', 'output'],\n",
       "     num_rows: 7825\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['instruction', 'output'],\n",
       "     num_rows: 7826\n",
       " })}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载并预处理数据集\n",
    "def load_and_process_data():\n",
    "    # 加载数据集\n",
    "    dataset = ds\n",
    "    \n",
    "    # 过滤过长样本（保留<2048 tokens）\n",
    "    def filter_long_samples(example):\n",
    "        tokens = tokenizer(example[\"instruction\"] + example[\"output\"], truncation=False)\n",
    "        return len(tokens[\"input_ids\"]) <= MAX_LENGTH\n",
    "    \n",
    "    filtered_dataset = dataset.filter(filter_long_samples)\n",
    "    \n",
    "    # 划分数据集（训练:验证:测试 = 8:1:1）\n",
    "    train_val_dataset = filtered_dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "    val_test_dataset = train_val_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "    \n",
    "    return {\n",
    "        \"train\": train_val_dataset[\"train\"],\n",
    "        \"validation\": val_test_dataset[\"train\"],\n",
    "        \"test\": val_test_dataset[\"test\"],\n",
    "    }\n",
    "\n",
    "datasets = load_and_process_data()\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ee4b9",
   "metadata": {},
   "source": [
    "# baseline评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f90ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70f87c3863c4a428ee4386c1d589184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# 加载tokenizer和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True,padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Qwen模型pad_token与eos_token相同\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,  # 使用BF16进行评估，平衡速度和精度\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5809f87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90a1c2d9d144678b3b169dcc00825ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 格式化数据集\n",
    "def format_dataset(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    # Qwen模型的指令格式（只包含指令部分，让模型生成回复）\n",
    "    formatted_text = f\"[INST] {instruction} [/INST]\"\n",
    "    return {\"text\": formatted_text, \"reference\": example[\"output\"]}\n",
    "\n",
    "# 加载并处理验证数据\n",
    "validation_dataset = datasets['test']\n",
    "validation_dataset = validation_dataset.map(format_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cfa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_valid_set = validation_dataset.shuffle().select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5448880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 生成配置\n",
    "generation_config = GenerationConfig(\n",
    "    # temperature=0.1,  # 低温度，提高确定性\n",
    "    max_new_tokens=1024,  # 最大生成长度\n",
    "    # top_p=0.9,\n",
    "    # top_k=40,\n",
    "    num_beams=1,  # 使用贪婪解码（更快）\n",
    "    # early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# 初始化BLEU评估器\n",
    "bleu = load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72019bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始在验证集上评估基线BLEU分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [1:41:52<00:00, 244.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "基线BLEU分数:\n",
      "bleu-1: 0.5731\n",
      "bleu-2: 0.4842\n",
      "bleu-3: 0.4253\n",
      "bleu-4: 0.3817\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './my_model/qwen2.5-7b-code-finetuned\\\\baseline_bleu_scores.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 保存结果\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaseline_bleu_scores.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m bleu_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     60\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2lc\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './my_model/qwen2.5-7b-code-finetuned\\\\baseline_bleu_scores.txt'"
     ]
    }
   ],
   "source": [
    "# 批量评估函数\n",
    "def evaluate_bleu(model, dataset, batch_size=4):\n",
    "    model.eval()  # 设置为评估模式\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    # 分批处理数据\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        \n",
    "        # 准备输入\n",
    "        inputs = tokenizer(\n",
    "            batch[\"text\"], \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=MAX_LENGTH\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # 生成回复\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "        \n",
    "        # 解码生成的回复\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.input_ids.shape[1]:],  # 只取生成的部分\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # 收集预测和参考\n",
    "        all_predictions.extend(generated_texts)\n",
    "        all_references.extend([[ref] for ref in batch[\"reference\"]])  # BLEU需要列表的列表\n",
    "    \n",
    "    # 计算BLEU分数（包括BLEU-1到BLEU-4）\n",
    "    results = {}\n",
    "    for n in range(1, 5):\n",
    "        results[f\"bleu-{n}\"] = bleu.compute(\n",
    "            predictions=all_predictions,\n",
    "            references=all_references,\n",
    "            max_order=n\n",
    "        )[\"bleu\"]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 执行评估\n",
    "print(\"开始在验证集上评估基线BLEU分数...\")\n",
    "bleu_results = evaluate_bleu(model, sample_valid_set)\n",
    "\n",
    "# 打印结果\n",
    "print(\"\\n基线BLEU分数:\")\n",
    "for key, value in bleu_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n基线分数已保存至: {os.path.join(OUTPUT_DIR, 'baseline_bleu_scores.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dc2ef",
   "metadata": {},
   "source": [
    "# 微调模型\n",
    "\n",
    "4位量化或者8位量化就是QLora形式，减少内存，但是会增大训练时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10253640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 格式化数据集（添加特殊token）\n",
    "def format_dataset(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    response = example[\"output\"]\n",
    "    # Qwen模型的指令格式\n",
    "    formatted_text = f\"[INST] {instruction} [/INST] {response}\"\n",
    "    return {\"text\": formatted_text}\n",
    "# 编码数据集\n",
    "def encode_dataset(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n",
    "\n",
    "# 格式化和编码\n",
    "for split in datasets:\n",
    "    datasets[split] = datasets[split].map(format_dataset)\n",
    "    datasets[split] = datasets[split].map(encode_dataset, batched=True)\n",
    "    datasets[split] = datasets[split].remove_columns([\"instruction\", \"output\", \"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3aad15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e0c648b76f4a849ce82d2a3ed41466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# !pip install bitsandbytes-cuda12x\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# 量化配置（INT4）\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "# )\n",
    "\n",
    "# LoRA配置\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# 数据收集器\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 按照量化配置加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    # quantization_config=bnb_config, # QLORA使用\n",
    "    torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,  # 使用BF16或FP16\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False\n",
    ")\n",
    "model.enable_input_require_grads()\n",
    "# 准备模型进行k-bit训练\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查模型的梯度状态\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter {name} has requires_grad=True\")\n",
    "    else:\n",
    "        print(f\"Parameter {name} has requires_grad=False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\d2lc\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0c3b76cfd64f9cb9b4a72afeb0b7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sets = datasets['train'].shuffle().select(range(1000))\n",
    "valid_sets = datasets['validation'].shuffle().select(range(100))\n",
    "\n",
    "# !pip install tensorboard\n",
    "model.train()\n",
    "# 训练参数配置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  # 等效batch_size=16\n",
    "    # fp16=True,\n",
    "    bf16=True,  # 使用BF16进行训练\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    dataloader_num_workers=4,  # 并行数据加载\n",
    "    gradient_checkpointing=True,  # 节省显存\n",
    "    remove_unused_columns=False,  # 保留所有列用于评估\n",
    ")\n",
    "\n",
    "\n",
    "# BLEU评估函数\n",
    "def compute_metrics(eval_preds):\n",
    "    bleu = load(\"bleu\")\n",
    "    predictions, labels = eval_preds\n",
    "    # 将预测和标签转换为文本\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # 标签中的-100需要替换为padding token id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # 计算BLEU-4\n",
    "    results = bleu.compute(predictions=decoded_preds, references=decoded_labels, max_order=4)\n",
    "    return {\"bleu-4\": results[\"bleu\"]}\n",
    "\n",
    "# 创建Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sets,\n",
    "    eval_dataset=valid_sets,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 保存LoRA权重（仅需约60-100MB）\n",
    "model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_lora_weights\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 在测试集上评估\n",
    "# test_results = trainer.evaluate(datasets[\"test\"])\n",
    "test_results = trainer.evaluate(sample_valid_set)\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
