{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2db36d0",
   "metadata": {},
   "source": [
    "# FAISS\n",
    "\n",
    "创建了一个 Datasets 仓库的 GitHub issues 和评论组成的数据集。在本节，我们将使用这些信息构建一个搜索引擎，帮助我们找到关于该库的最相关的 issue 的答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92386cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8581c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "092f38d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f072ea62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = issues_dataset.map(lambda x:{k:v for k,v in x.items() if k in ['html_url','title']},remove_columns=issues_dataset.column_names)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2401da",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75c2144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>[Cool, I think we can do both :), @lhoestq now...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>[Hi ! I guess the caching mechanism should hav...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplit...</td>\n",
       "      <td>[I tried `unshuffled_original_da` and it is al...</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>load_dataset using default cache on Windows ca...</td>\n",
       "      <td>[Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...</td>\n",
       "      <td>## Describe the bug\\r\\nStandard process to dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>to_tf_dataset keeps a reference to the open da...</td>\n",
       "      <td>[I did some investigation and, as it seems, th...</td>\n",
       "      <td>To reproduce:\\r\\n```python\\r\\nimport datasets ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1  Backwards compatibility broken for cached data...   \n",
       "2  OSCAR unshuffled_original_ko: NonMatchingSplit...   \n",
       "3  load_dataset using default cache on Windows ca...   \n",
       "4  to_tf_dataset keeps a reference to the open da...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  [Cool, I think we can do both :), @lhoestq now...   \n",
       "1  [Hi ! I guess the caching mechanism should hav...   \n",
       "2  [I tried `unshuffled_original_da` and it is al...   \n",
       "3  [Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...   \n",
       "4  [I did some investigation and, as it seems, th...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "2  ## Describe the bug\\r\\n\\r\\nCannot download OSC...  \n",
       "3  ## Describe the bug\\r\\nStandard process to dow...  \n",
       "4  To reproduce:\\r\\n```python\\r\\nimport datasets ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1877ceb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments'][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2247230c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                    Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 explode() 将这些评论中的每一条都展开成为一行\n",
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "790186d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b225fbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27232f0990648b4bfadeca73553d3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e221bcbebab4e3686641ef05ad6b63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comments_length 列来存放每条评论的字数\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")\n",
    "# 过滤掉简短的评论，其中通常包括“cc @lewtun”或“谢谢！”之类与我们的搜索引擎无关的内容\n",
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f5ffa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f0b92876014f128649f76e8416df0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 issue 标题、描述和评论构建一个新的 text 列\n",
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f8ca3",
   "metadata": {},
   "source": [
    "# 创建文本嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa2c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9622ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293aec3e92104bcd8149d1126cf8ca01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# GitHub issue 中的每一条记录转化为一个单一的向量，所以我们需要以某种方式“池化（pool）”或平均每个词的嵌入向量。\n",
    "# 一种流行的方法是在我们模型的输出上进行 CLS 池化 ，我们只需要收集 [CLS] token 的的最后一个隐藏状态。\n",
    "# 以下函数实现了这个功能，为什么只需要CLS，是因为模型最后一层的时候CLS已经有了整个句子的含义\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0] #[:,0]   模型最后一层的，所有行的第一个token也就是[CLS]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "# 测试一篇text\n",
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape\n",
    "embedding_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\":get_embeddings(x['text']).detach().cpu().numpy()[0]}\n",
    ")\n",
    "# 文本嵌入转换为 NumPy 数组——这是因为当我们尝试使用 FAISS 搜索它们时，Datasets 需要这种格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6079af7e",
   "metadata": {},
   "source": [
    "# 使用 FAISS 进行高效的相似性搜索\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bab24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda uninstall faiss-cpu\n",
    "# !pip uninstall faiss-cpu\n",
    "# !conda install -c pytorch faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19905371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['APPROX_TOPK_BUCKETS_B16_D2', 'APPROX_TOPK_BUCKETS_B32_D2', 'APPROX_TOPK_BUCKETS_B8_D2', 'APPROX_TOPK_BUCKETS_B8_D3', 'AdditiveCoarseQuantizer', 'AdditiveQuantizer', 'AlignedTableFloat32', 'AlignedTableFloat32_round_capacity', 'AlignedTableUint16', 'AlignedTableUint16_round_capacity', 'AlignedTableUint8', 'AlignedTableUint8_round_capacity', 'AlignedTable_to_array', 'ArrayInvertedLists', 'AutoTuneCriterion', 'BinaryInvertedListScanner', 'BitstringReader', 'BitstringWriter', 'BlockInvertedLists', 'BufferList', 'BufferedIOReader', 'BufferedIOWriter', 'ByteVector', 'ByteVectorVector', 'CMax_float_partition_fuzzy', 'CMax_uint16_partition_fuzzy', 'CMin_float_partition_fuzzy', 'CMin_uint16_partition_fuzzy', 'CenteringTransform', 'CharVector', 'Cloner', 'Clustering', 'Clustering1D', 'ClusteringIterationStats', 'ClusteringIterationStatsVector', 'ClusteringParameters', 'CodePacker', 'CodePackerFlat', 'CodeSet', 'CombinerRangeKNNfloat', 'CombinerRangeKNNint16', 'ComputeCodesAddCentroidsLUT0MemoryPool', 'ComputeCodesAddCentroidsLUT1MemoryPool', 'ConcatenatedInvertedLists', 'DirectMap', 'DirectMapAdd', 'DistanceComputer', 'DoubleVector', 'DummyResultHandler', 'EXACT_TOPK', 'Embedding', 'EnumeratedVectors', 'FAISS_VERSION_MAJOR', 'FAISS_VERSION_MINOR', 'FAISS_VERSION_PATCH', 'FFN', 'FastScanStats', 'FileIOReader', 'FileIOWriter', 'FlatCodesDistanceComputer', 'Float32Vector', 'Float32VectorVector', 'Float64Vector', 'FloatVector', 'FloatVectorVector', 'HNSW', 'HNSWStats', 'HNSW_shrink_neighbor_list', 'HStackInvertedLists', 'IDSelector', 'IDSelectorAll', 'IDSelectorAnd', 'IDSelectorArray', 'IDSelectorBatch', 'IDSelectorBitmap', 'IDSelectorNot', 'IDSelectorOr', 'IDSelectorRange', 'IDSelectorTranslated', 'IDSelectorXOr', 'IOReader', 'IOWriter', 'IO_FLAG_MMAP', 'IO_FLAG_ONDISK_SAME_DIR', 'IO_FLAG_PQ_SKIP_SDC_TABLE', 'IO_FLAG_READ_ONLY', 'IO_FLAG_SKIP_IVF_DATA', 'IO_FLAG_SKIP_PRECOMPUTE_TABLE', 'IO_FLAG_SKIP_STORAGE', 'ITQMatrix', 'ITQTransform', 'IVFFastScanStats', 'IVFPQSearchParameters', 'IVFSearchParameters', 'IcmEncoder', 'IcmEncoderFactory', 'Index', 'Index2Layer', 'IndexAdditiveQuantizer', 'IndexAdditiveQuantizerFastScan', 'IndexBinary', 'IndexBinaryFlat', 'IndexBinaryFromFloat', 'IndexBinaryHNSW', 'IndexBinaryHash', 'IndexBinaryHashStats', 'IndexBinaryIDMap', 'IndexBinaryIDMap2', 'IndexBinaryIVF', 'IndexBinaryMultiHash', 'IndexBinaryReplicas', 'IndexBinaryShards', 'IndexFastScan', 'IndexFlat', 'IndexFlat1D', 'IndexFlatCodes', 'IndexFlatIP', 'IndexFlatL2', 'IndexHNSW', 'IndexHNSW2Level', 'IndexHNSWCagra', 'IndexHNSWFlat', 'IndexHNSWPQ', 'IndexHNSWSQ', 'IndexIDMap', 'IndexIDMap2', 'IndexIVF', 'IndexIVFAdditiveQuantizer', 'IndexIVFAdditiveQuantizerFastScan', 'IndexIVFFastScan', 'IndexIVFFlat', 'IndexIVFFlatDedup', 'IndexIVFIndependentQuantizer', 'IndexIVFInterface', 'IndexIVFLocalSearchQuantizer', 'IndexIVFLocalSearchQuantizerFastScan', 'IndexIVFPQ', 'IndexIVFPQFastScan', 'IndexIVFPQR', 'IndexIVFPQStats', 'IndexIVFProductLocalSearchQuantizer', 'IndexIVFProductLocalSearchQuantizerFastScan', 'IndexIVFProductResidualQuantizer', 'IndexIVFProductResidualQuantizerFastScan', 'IndexIVFResidualQuantizer', 'IndexIVFResidualQuantizerFastScan', 'IndexIVFScalarQuantizer', 'IndexIVFSpectralHash', 'IndexIVFStats', 'IndexLSH', 'IndexLattice', 'IndexLocalSearchQuantizer', 'IndexLocalSearchQuantizerFastScan', 'IndexNNDescent', 'IndexNNDescentFlat', 'IndexNSG', 'IndexNSGFlat', 'IndexNSGPQ', 'IndexNSGSQ', 'IndexNeuralNetCodec', 'IndexPQ', 'IndexPQFastScan', 'IndexPQStats', 'IndexPreTransform', 'IndexProductLocalSearchQuantizer', 'IndexProductLocalSearchQuantizerFastScan', 'IndexProductResidualQuantizer', 'IndexProductResidualQuantizerFastScan', 'IndexProxy', 'IndexQINCo', 'IndexRandom', 'IndexRefine', 'IndexRefineFlat', 'IndexRefineSearchParameters', 'IndexReplicas', 'IndexResidual', 'IndexResidualQuantizer', 'IndexResidualQuantizerFastScan', 'IndexRowwiseMinMax', 'IndexRowwiseMinMaxBase', 'IndexRowwiseMinMaxFP16', 'IndexScalarQuantizer', 'IndexShards', 'IndexShardsIVF', 'IndexSplitVectors', 'Int16Vector', 'Int32Tensor2D', 'Int32Vector', 'Int32VectorVector', 'Int64Vector', 'Int64VectorVector', 'Int8Vector', 'IntVector', 'InterruptCallback', 'InterruptCallback_check', 'InterruptCallback_clear_instance', 'InterruptCallback_get_period_hint', 'InterruptCallback_is_interrupted', 'IntersectionCriterion', 'InvertedListScanner', 'InvertedLists', 'InvertedListsIOHook', 'InvertedListsIOHook_add_callback', 'InvertedListsIOHook_lookup', 'InvertedListsIOHook_lookup_classname', 'InvertedListsIOHook_print_callbacks', 'InvertedListsIterator', 'InvertedListsPtrVector', 'Kmeans', 'LSQTimer', 'LSQTimerScope', 'Level1Quantizer', 'Linear', 'LinearTransform', 'LocalSearchCoarseQuantizer', 'LocalSearchQuantizer', 'LongLongVector', 'LongVector', 'LongVectorVector', 'METRIC_ABS_INNER_PRODUCT', 'METRIC_BrayCurtis', 'METRIC_Canberra', 'METRIC_INNER_PRODUCT', 'METRIC_Jaccard', 'METRIC_JensenShannon', 'METRIC_L1', 'METRIC_L2', 'METRIC_Linf', 'METRIC_Lp', 'METRIC_NaNEuclidean', 'MapInt64ToInt64', 'MapLong2Long', 'MaskedInvertedLists', 'MatrixStats', 'MultiIndexQuantizer', 'MultiIndexQuantizer2', 'NNDescent', 'NSG', 'NSG_Graph_int', 'NegativeDistanceComputer', 'Neighbor', 'NeuralNetCodec', 'Nhood', 'NormalizationTransform', 'OPQMatrix', 'OneRecallAtRCriterion', 'OperatingPoint', 'OperatingPointVector', 'OperatingPoints', 'PCAMatrix', 'PQDecoder16', 'PQDecoder8', 'PQDecoderGeneric', 'PQEncoder16', 'PQEncoder8', 'PQEncoderGeneric', 'ParameterRange', 'ParameterRangeVector', 'ParameterSpace', 'PartitionStats', 'PermutationObjective', 'PolysemousTraining', 'ProductAdditiveQuantizer', 'ProductLocalSearchQuantizer', 'ProductQuantizer', 'ProductResidualQuantizer', 'ProgressiveDimClustering', 'ProgressiveDimClusteringParameters', 'ProgressiveDimIndexFactory', 'PyCallbackIDSelector', 'PyCallbackIOReader', 'PyCallbackIOWriter', 'PythonInterruptCallback', 'PythonInterruptCallback_reset', 'QINCo', 'QINCoStep', 'Quantizer', 'RandomGenerator', 'RandomRotationMatrix', 'RangeQueryResult', 'RangeSearchPartialResult', 'RangeSearchPartialResult_merge', 'RangeSearchResult', 'ReadOnlyInvertedLists', 'RefineBeamLUTMemoryPool', 'RefineBeamMemoryPool', 'RemapDimensionsTransform', 'Repeat', 'RepeatVector', 'Repeats', 'ReproduceDistancesObjective', 'ReproduceDistancesObjective_compute_mean_stdev', 'ReproduceDistancesObjective_sqr', 'ResidualCoarseQuantizer', 'ResidualQuantizer', 'ResultHeap', 'SHARED_PTR_DISOWN', 'SIMDResultHandler', 'SIMDResultHandlerToFloat', 'ScalarQuantizer', 'SearchParameters', 'SearchParametersHNSW', 'SearchParametersIVF', 'SearchParametersPQ', 'SearchParametersPreTransform', 'SearchParametersResidualCoarseQuantizer', 'SimulatedAnnealingOptimizer', 'SimulatedAnnealingParameters', 'SliceInvertedLists', 'SlidingIndexWindow', 'SplitMix64RandomGenerator', 'StopWordsInvertedLists', 'StoreResultHandler', 'SwigPyIterator', 'Tensor2D', 'ThreadedIndexBase', 'ThreadedIndexBaseBinary', 'TimeoutCallback', 'TimeoutCallback_reset', 'TimeoutGuard', 'UInt16Vector', 'UInt32Vector', 'UInt64Vector', 'UInt8Vector', 'UInt8VectorVector', 'Uint64Vector', 'VERSION_STRING', 'VStackInvertedLists', 'VectorIOReader', 'VectorIOWriter', 'VectorTransform', 'VectorTransformVector', 'Version', 'VisitedTable', 'ZnSphereCodec', 'ZnSphereCodecAlt', 'ZnSphereCodecRec', 'ZnSphereSearch', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_swigfaiss', 'add_ref_in_constructor', 'add_ref_in_function', 'add_ref_in_method', 'add_ref_in_method_explicit_own', 'add_to_referenced_objects', 'aq_estimate_norm_scale', 'aq_quantize_LUT_and_bias', 'array', 'array_conversions', 'array_to_AlignedTable', 'base_prefix', 'beam_search_encode_step', 'beam_search_encode_step_tab', 'binary_to_real', 'bincode_hist', 'bitvec_print', 'bitvec_shuffle', 'bitvecs2fvecs', 'bucket_sort', 'bvec_checksum', 'bvecs_checksum', 'byte_rand', 'cast_integer_to_float_ptr', 'cast_integer_to_idx_t_ptr', 'cast_integer_to_int_ptr', 'cast_integer_to_uint8_ptr', 'cast_integer_to_void_ptr', 'check_compatible_for_merge', 'check_openmp', 'checksum', 'class_wrappers', 'clone_Quantizer', 'clone_binary_index', 'clone_index', 'compute_PQ_dis_tables_dsub2', 'compute_codes_add_centroids_mp_lut0', 'compute_codes_add_centroids_mp_lut1', 'copy_array_to_AlignedTable', 'copy_array_to_vector', 'crosshamming_count_thres', 'cvar', 'depr_prefix', 'deprecated_name_map', 'deserialize_index', 'deserialize_index_binary', 'downcast_AdditiveQuantizer', 'downcast_IndexBinary', 'downcast_InvertedLists', 'downcast_Quantizer', 'downcast_VectorTransform', 'downcast_index', 'eval_intersection', 'extra_wrappers', 'extract_index_ivf', 'float_maxheap_array_t', 'float_minheap_array_t', 'float_rand', 'float_randn', 'fourcc', 'fourcc_inv', 'fourcc_inv_printable', 'fvec2bitvec', 'fvec_L1', 'fvec_L2sqr', 'fvec_L2sqr_batch_4', 'fvec_L2sqr_by_idx', 'fvec_L2sqr_ny', 'fvec_L2sqr_ny_nearest', 'fvec_L2sqr_ny_nearest_y_transposed', 'fvec_L2sqr_ny_transposed', 'fvec_Linf', 'fvec_add', 'fvec_argsort', 'fvec_argsort_parallel', 'fvec_inner_product', 'fvec_inner_product_batch_4', 'fvec_inner_products_by_idx', 'fvec_inner_products_ny', 'fvec_madd', 'fvec_madd_and_argmin', 'fvec_norm_L2sqr', 'fvec_norms_L2', 'fvec_norms_L2sqr', 'fvec_renorm_L2', 'fvec_sub', 'fvecs2bitvecs', 'fvecs_maybe_subsample', 'generalized_hammings_knn_hc', 'get_compile_options', 'get_cycles', 'get_extra_distance_computer', 'get_invlist_range', 'get_mem_usage_kb', 'get_num_gpus', 'get_version', 'getmillisecs', 'gpu_profiler_start', 'gpu_profiler_stop', 'gpu_sync_all_devices', 'gpu_wrappers', 'greedy_update_nearest', 'hamdis_tab_ham_bytes', 'hamming_count_thres', 'hamming_range_search', 'hammings', 'hammings_knn', 'hammings_knn_hc', 'hammings_knn_mc', 'has_AVX2', 'has_AVX512', 'has_SVE', 'hash_bytes', 'hashtable_int64_to_int64_add', 'hashtable_int64_to_int64_init', 'hashtable_int64_to_int64_lookup', 'imbalance_factor', 'index_binary_factory', 'index_cpu_to_all_gpus', 'index_cpu_to_gpu_multiple_py', 'index_cpu_to_gpus_list', 'index_factory', 'initialize_IVFPQ_precomputed_table', 'inner_product_to_L2sqr', 'inspect', 'instruction_sets', 'int64_rand', 'int64_rand_max', 'int_maxheap_array_t', 'int_minheap_array_t', 'is_similarity_metric', 'ivec_checksum', 'ivec_hist', 'ivf_residual_add_from_flat_codes', 'ivf_residual_from_quantizer', 'kmax', 'kmeans1d', 'kmeans_clustering', 'kmin', 'knn', 'knn_L2sqr', 'knn_L2sqr_by_idx', 'knn_extra_metrics', 'knn_gpu', 'knn_hamming', 'knn_inner_product', 'knn_inner_products_by_idx', 'lo_build', 'lo_listno', 'lo_offset', 'loaded', 'loader', 'logger', 'logging', 'lrand', 'match_hamming_thres', 'matrix_bucket_sort_inplace', 'matrix_qr', 'memcpy', 'merge_into', 'merge_knn_results', 'merge_knn_results_CMax', 'merge_knn_results_CMin', 'merge_result_table_with', 'normalize_L2', 'np', 'obj', 'omp_get_max_threads', 'omp_set_num_threads', 'opt_env_variable_name', 'opt_level', 'os', 'pack_bitstrings', 'pairwise_L2sqr', 'pairwise_distance_gpu', 'pairwise_distances', 'pairwise_extra_distances', 'pairwise_indexed_L2sqr', 'pairwise_indexed_inner_product', 'platform', 'popcount32', 'popcount64', 'quantize_LUT_and_bias', 'rand', 'rand_perm', 'rand_perm_splitmix64', 'rand_smooth_vectors', 'randint', 'randn', 'range_search_L2sqr', 'range_search_inner_product', 'range_search_with_parameters', 'range_search_with_parameters_c', 'ranklist_handle_ties', 'ranklist_intersection_size', 'read_InvertedLists', 'read_ProductQuantizer', 'read_VectorTransform', 'read_index', 'read_index_binary', 'real_to_binary', 'refine_beam_LUT_mp', 'refine_beam_mp', 'reflection', 'rev_swig_ptr', 'round_uint8_per_column', 'round_uint8_per_column_multi', 'search_and_return_centroids', 'search_centroid', 'search_from_candidate_unbounded', 'search_from_candidates', 'search_with_parameters', 'search_with_parameters_c', 'serialize_index', 'serialize_index_binary', 'set_invlist_range', 'simd16uint16', 'simd_histogram_16', 'simd_histogram_8', 'sizeof_long', 'smawk', 'storage_distance_computer', 'subprocess', 'supported_instruction_sets', 'swig_ptr', 'swig_version', 'swigfaiss', 'symbol', 'sys', 'the_class', 'this_module', 'try_extract_index_ivf', 'unpack_bitstrings', 'vector_float_to_array', 'vector_name_map', 'vector_to_array', 'warnings', 'write_InvertedLists', 'write_ProductQuantizer', 'write_VectorTransform', 'write_index', 'write_index_binary']\n",
      "d:\\Anaconda\\envs\\d2lc\\lib\\site-packages\\faiss\\__init__.py\n",
      "True\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "!python -c \"import faiss; print(dir(faiss))\"\n",
    "print(faiss.__file__)  # 应该输出类似 D:\\Anaconda\\envs\\d2lc\\lib\\site-packages\\faiss\\__init__.py\n",
    "\n",
    "# 创建随机向量\n",
    "d = 64  # 向量维度\n",
    "n = 1000  # 向量数量\n",
    "xb = np.random.random((n, d)).astype('float32')\n",
    "\n",
    "# 创建 IndexFlatL2 索引\n",
    "index = faiss.IndexFlatL2(d)  # 这行可能会出错\n",
    "print(index.is_trained)  # 应该输出 True\n",
    "\n",
    "# 添加向量到索引\n",
    "index.add(xb)\n",
    "print(index.ntotal)  # 应该输出 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2108c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0673e020b37e463fb160dad09bc254e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
      "\n",
      "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "SCORE: 25.505016326904297\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
      "You can now use them offline\n",
      "```python\n",
      "datasets = load_dataset('text', data_files=data_files)\n",
      "```\n",
      "\n",
      "We'll do a new release soon\n",
      "SCORE: 24.555530548095703\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
      "\n",
      "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
      "\n",
      "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
      "\n",
      "----------\n",
      "\n",
      "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "\n",
      "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
      "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
      "```python\n",
      "load_dataset(\"./my_dataset\")\n",
      "```\n",
      "and the dataset script will generate your dataset once and for all.\n",
      "\n",
      "----------\n",
      "\n",
      "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
      "cf #1724 \n",
      "SCORE: 24.14898681640625\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
      "> \n",
      "> 1. (online machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_dataset(...)\n",
      "> \n",
      "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> 2. copy the dir from online to the offline machine\n",
      "> \n",
      "> 3. (offline machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> \n",
      "> \n",
      "> HTH.\n",
      "\n",
      "\n",
      "SCORE: 22.894004821777344\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
      "1. (online machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_dataset(...)\n",
      "data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "```\n",
      "2. copy the dir from online to the offline machine\n",
      "3. (offline machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "```\n",
      "\n",
      "HTH.\n",
      "SCORE: 22.406658172607422\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 FAISS index（索引）\n",
    "embedding_dataset.add_faiss_index(column=\"embeddings\")\n",
    "# 使用 Dataset.get_nearest_examples() 函数进行最近邻居查找\n",
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape\n",
    "\n",
    "scores, samples = embedding_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")\n",
    "# 收集并排序这五个最接近结果\n",
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.head()\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "# 遍历前几行来查看我们的查询与评论的匹配程度如何\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
