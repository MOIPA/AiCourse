{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c605d777",
   "metadata": {},
   "source": [
    "### åŸºäºTransformersåº“å…¨é‡å¾®è°ƒå¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f687896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ç”¨æˆ‘ä»¬æä¾›çš„ä¸¤å¥è¯ä½œä¸ºæ ·æœ¬ï¼Œæ¥è®­ç»ƒå¥å­åˆ†ç±»å™¨\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# å’Œä¹‹å‰ä¸€æ ·\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# å¸¦åˆ†ç±»å¤´çš„æ¨¡å‹\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# æ–°å¢éƒ¨åˆ†\n",
    "batch[\"labels\"] = torch.tensor([1, 1])  # ä¸¤ä¸ªå¥å­éƒ½æ˜¯positiveï¼Œç»™1å’Œ1\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc63f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¸‹è½½å®é™…æ•°æ®é›†å¹¶è§‚å¯Ÿæ•°æ®é›†\n",
    "\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf403d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets['train']\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "711a60fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5892ce1",
   "metadata": {},
   "source": [
    "### é¢„å¤„ç†æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aad61ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sentence1 = tokenizer(raw_train_dataset[0]['sentence1'])\n",
    "sentence2 = tokenizer(raw_train_dataset[0]['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790f1290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56796cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102]\n",
      "['[CLS]', 'am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '\"', 'the', 'witness', '\"', ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]', 'referring', 'to', 'him', 'as', 'only', '\"', 'the', 'witness', '\"', ',', 'am', '##ro', '##zi', 'accused', 'his', 'brother', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# è¿™é‡Œè¯´æ˜ä¸‹ token_type_ids ï¼Œä¸€ä¸ªæ ·æœ¬æœ‰ä¸¤ä¸ªå¥å­æ—¶ï¼Œtoken_type_idså¯ä»¥è¡¨æ˜å“ªäº›tokenæ˜¯å¥å­1ï¼Œå“ªäº›æ˜¯å¥å­2\n",
    "test = tokenizer(raw_train_dataset[0]['sentence1'],raw_train_dataset[0]['sentence2'],padding=True,truncation=True)\n",
    "print(test['input_ids'])\n",
    "print(tokenizer.convert_ids_to_tokens(test['input_ids']))\n",
    "# å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªå¥å­ä¹‹é—´åŠ å…¥äº†SEPä½œä¸ºåˆ†å‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c31f189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fe33d43d9c4ec197eadeafc6bc0259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# å¤„ç†æ•´ä¸ªæ•°æ®é›†\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "# ä»¥ä¸Šè¿™ç§æ–¹å¼ä¼šå°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜ï¼Œæ•ˆç‡ä¸é«˜ï¼Œé‡‡ç”¨ä¸‹é¢è¿™ç§æ–¹å¼\n",
    "# å®šä¹‰å¤„ç†å‡½æ•°ï¼Œæ¥å—æ•°æ®å­—å…¸\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True) \n",
    "tokenized_dataset = raw_datasets.map(tokenize_function,batched=True) # map() æ–¹æ³•çš„å·¥ä½œåŸç†æ˜¯ä½¿ç”¨ä¸€ä¸ªå‡½æ•°å¤„ç†æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ \n",
    "# batched=True  æŒ‰æ‰¹æ¬¡å¤„ç†æ•°æ®ï¼Œå°†å¤šä¸ªæ ·æœ¬æ‰“åŒ…æˆä¸€ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰ï¼Œä¸€æ¬¡æ€§ä¼ å…¥tokenize_functionå¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a52f420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset # æ³¨æ„åˆ°è¿”å›çš„æ•°æ®é›†æ˜¯æ²¡æœ‰paddingçš„ï¼Œä¸”ç›¸å½“äºç»™æ¯ä¸ªå­æ•°æ®é›†æ·»åŠ äº†ä¸‰ä¸ªå­—æ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8e8f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [1, 0, 1, 0, 1, 1, 0, 1], 'input_ids': [[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], [101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102], [101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102, 2006, 2238, 2184, 1010, 1996, 2911, 1005, 1055, 5608, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 1010, 5378, 1996, 14792, 2005, 5096, 1012, 102], [101, 2105, 6021, 19481, 13938, 2102, 1010, 21628, 6661, 2020, 2039, 2539, 16653, 1010, 2030, 1018, 1012, 1018, 1003, 1010, 2012, 1037, 1002, 1018, 1012, 5179, 1010, 2383, 3041, 2275, 1037, 2501, 2152, 1997, 1037, 1002, 1018, 1012, 5401, 1012, 102, 21628, 6661, 5598, 2322, 16653, 1010, 2030, 1018, 1012, 1020, 1003, 1010, 2000, 2275, 1037, 2501, 5494, 2152, 2012, 1037, 1002, 1018, 1012, 5401, 1012, 102], [101, 1996, 4518, 3123, 1002, 1016, 1012, 2340, 1010, 2030, 2055, 2340, 3867, 1010, 2000, 2485, 5958, 2012, 1002, 2538, 1012, 4868, 2006, 1996, 2047, 2259, 4518, 3863, 1012, 102, 18720, 1004, 1041, 13058, 1012, 6661, 5598, 1002, 1015, 1012, 6191, 2030, 1022, 3867, 2000, 1002, 2538, 1012, 6021, 2006, 1996, 2047, 2259, 4518, 3863, 2006, 5958, 1012, 102], [101, 6599, 1999, 1996, 2034, 4284, 1997, 1996, 2095, 3333, 2321, 3867, 2013, 1996, 2168, 2558, 1037, 2095, 3041, 1012, 102, 2007, 1996, 9446, 5689, 2058, 5954, 1005, 1055, 2194, 1010, 6599, 1996, 2034, 4284, 1997, 1996, 2095, 3333, 2321, 3867, 2013, 1996, 2168, 2558, 1037, 2095, 3041, 1012, 102], [101, 1996, 17235, 2850, 4160, 2018, 1037, 4882, 5114, 1997, 2459, 1012, 2676, 1010, 2030, 1015, 1012, 1016, 3867, 1010, 5494, 2012, 1015, 1010, 19611, 1012, 2321, 2006, 5958, 1012, 102, 1996, 6627, 1011, 17958, 17235, 2850, 4160, 12490, 1012, 11814, 2594, 24356, 2382, 1012, 4805, 2685, 1010, 2030, 1016, 1012, 5840, 3867, 1010, 2000, 1015, 1010, 19611, 1012, 2321, 1012, 102], [101, 1996, 4966, 1011, 10507, 2050, 2059, 12068, 2000, 1996, 2110, 4259, 2457, 1012, 102, 1996, 4966, 10507, 2050, 12068, 2008, 3247, 2000, 1996, 1057, 1012, 1055, 1012, 4259, 2457, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "[50, 59, 47, 67, 59, 50, 62, 32]\n",
      "[67, 67, 67, 67, 67, 67, 67, 67]\n"
     ]
    }
   ],
   "source": [
    "# å°†æ¯ä¸ªbatchå¥å­é•¿åº¦å¡«å……åˆ°æ­£ç¡®é•¿åº¦\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "samples = tokenized_dataset['train'][:8] # è®¿é—®å‰å…«ä¸ªå…ƒç´ \n",
    "samples = {k:v for k,v in samples.items() if k not in ['idx','sentence1','sentence2']}\n",
    "print(samples)\n",
    "# æŸ¥çœ‹è¿™8ä¸ªæ ·æœ¬çš„æ¯ä¸ªé•¿åº¦\n",
    "print([len(x) for x in samples['input_ids']])\n",
    "# ä½¿ç”¨collatorè¿›è¡Œæ•°æ®å¡«å……\n",
    "batch = data_collator(samples)\n",
    "\n",
    "print([len(x) for x in batch['input_ids']]) # å¯ä»¥çœ‹åˆ°å…¨éƒ½å¡«å……åˆ°äº†æœ€å¤§é•¿åº¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc2bce1",
   "metadata": {},
   "source": [
    "### è¿›è¡Œå¾®è°ƒï¼ˆTransformer.Trainerï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b19f70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c826b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c huggingface transformers accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9339dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\") # åŠ è½½è¶…å‚æ•°é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5465d79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ¨¡å‹\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebe06e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tassa\\AppData\\Local\\Temp\\ipykernel_23376\\2497664513.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model\n",
    "    ,training_args\n",
    "    ,train_dataset=tokenized_datasets['train']\n",
    "    ,eval_dataset=tokenized_datasets['validation']\n",
    "    ,data_collator=data_collator\n",
    "    ,tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b035268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "192e4ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3743a2ebfbb84f56be746462269ed618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5034, 'grad_norm': 1.5853326320648193, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n",
      "{'loss': 0.2727, 'grad_norm': 23.290136337280273, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n",
      "{'train_runtime': 126.259, 'train_samples_per_second': 87.154, 'train_steps_per_second': 10.906, 'train_loss': 0.3264000808837713, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3264000808837713, metrics={'train_runtime': 126.259, 'train_samples_per_second': 87.154, 'train_steps_per_second': 10.906, 'total_flos': 405114969714960.0, 'train_loss': 0.3264000808837713, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef754b1",
   "metadata": {},
   "source": [
    "### æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0acae74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32a0e7ee0c94ee08ffd10930ef1a828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6b7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0\n",
      " 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
      " 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1\n",
      " 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0\n",
      " 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1\n",
      " 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0\n",
      " 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
      " 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
      " 1]\n",
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212880f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "preds = np.argmax(predictions.predictions,axis=1)\n",
    "preds # å®é™…é¢„æµ‹ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32b63ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e648af01ae14b518962f1901aace104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8455882352941176, 'f1': 0.8930390492359932}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¯„ä¼°é¢„æµ‹å’ŒçœŸå®ç»“æœå¯¹æ¯”\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\",\"mrpc\")\n",
    "metric.compute(predictions=preds,references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60833038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“åŒ…è¯„ä¼°å‡½æ•°\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")  # åŠ è½½ä¸ GLUE åŸºå‡†æµ‹è¯•ä¸­ MRPC ä»»åŠ¡ç›¸å…³çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆMetricï¼‰æ˜¯ä¸€ç»„è®¡ç®—é€»è¾‘ï¼Œç”¨äºè¡¡é‡æ¨¡å‹é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾çš„åŒ¹é…ç¨‹åº¦ï¼ˆå¦‚è®¡ç®—predictionså’Œreferencesçš„å‡†ç¡®ç‡ï¼‰\n",
    "    # MRPCï¼šå¾®è½¯ç ”ç©¶é‡Šä¹‰è¯­æ–™åº“ï¼ˆMicrosoft Research Paraphrase Corpusï¼‰ï¼Œä»»åŠ¡ä¸ºåˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦ä¸ºé‡Šä¹‰ï¼ˆäºŒåˆ†ç±»ä»»åŠ¡ï¼‰ã€‚\n",
    "    # å¯¹åº”æŒ‡æ ‡ï¼šMRPC ä»»åŠ¡çš„å®˜æ–¹è¯„ä¼°æŒ‡æ ‡æ˜¯å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰å’ŒF1 åˆ†æ•°ï¼ˆF1-Scoreï¼‰\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a3cc55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\d2lc\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\tassa\\AppData\\Local\\Temp\\ipykernel_23376\\1708427569.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ä¸ºäº†æŸ¥çœ‹æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸæ—¶çš„å¥½åï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ compute_metrics() å‡½æ•°å®šä¹‰ä¸€ä¸ªæ–°çš„ Trainer\n",
    "training_args = TrainingArguments(\"test-trainer\",evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model\n",
    "    ,training_args\n",
    "    ,train_dataset=tokenized_datasets['train']\n",
    "    ,eval_dataset=tokenized_datasets['validation']\n",
    "    ,data_collator=data_collator\n",
    "    ,tokenizer=tokenizer\n",
    "    ,compute_metrics=compute_metrics # æ·»åŠ è‡ªå·±çš„è¯„ä¼°æ–¹æ³•ï¼Œæ¯ä¸ªepocè¯„ä¼°ä¸€æ¬¡\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a640009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987ef9a42b1f41d9b30e095a4d395ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c3ea19b92348f9865a861a40591744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49888285994529724, 'eval_accuracy': 0.7377450980392157, 'eval_f1': 0.8366412213740458, 'eval_runtime': 4.9545, 'eval_samples_per_second': 82.349, 'eval_steps_per_second': 10.294, 'epoch': 1.0}\n",
      "{'loss': 0.5846, 'grad_norm': 11.974019050598145, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91aa3b8c7a15490582ac915e88d92e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4442324638366699, 'eval_accuracy': 0.8186274509803921, 'eval_f1': 0.8724137931034482, 'eval_runtime': 4.6248, 'eval_samples_per_second': 88.219, 'eval_steps_per_second': 11.027, 'epoch': 2.0}\n",
      "{'loss': 0.4481, 'grad_norm': 1.7519233226776123, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a51441fd844e74b8300a3d78048b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5072584748268127, 'eval_accuracy': 0.8382352941176471, 'eval_f1': 0.8842105263157894, 'eval_runtime': 3.2548, 'eval_samples_per_second': 125.352, 'eval_steps_per_second': 15.669, 'epoch': 3.0}\n",
      "{'train_runtime': 138.4389, 'train_samples_per_second': 79.486, 'train_steps_per_second': 9.947, 'train_loss': 0.4726305970612348, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.4726305970612348, metrics={'train_runtime': 138.4389, 'train_samples_per_second': 79.486, 'train_steps_per_second': 9.947, 'total_flos': 405114969714960.0, 'train_loss': 0.4726305970612348, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d3605",
   "metadata": {},
   "source": [
    "# ä¸€ä¸ªå®Œæ•´çš„å…¨é‡å¾®è°ƒè®­ç»ƒï¼ˆPytorchæ‰‹åŠ¨ç‰ˆï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡å‹æ•°æ®é¢„å¤„ç†ï¼Œåˆ é™¤ä¸ç”¨çš„å­—æ®µï¼Œæ¨¡å‹è®­ç»ƒæ—¶é»˜è®¤çš„æ ‡ç­¾å­—æ®µæ˜¯labels\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1','sentence2','idx'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label','labels')\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65cd781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': torch.Size([8]), 'input_ids': torch.Size([8, 70]), 'token_type_ids': torch.Size([8, 70]), 'attention_mask': torch.Size([8, 70])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'],shuffle=True,batch_size=8,collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'],shuffle=True,batch_size=8,collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# check ä¸€ä¸‹\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "print({k:v.shape for k,v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a394322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.7860, grad_fn=<NllLossBackward0>), logits=tensor([[0.2098, 0.5178],\n",
      "        [0.2238, 0.4991],\n",
      "        [0.2280, 0.5289],\n",
      "        [0.1553, 0.5280],\n",
      "        [0.2202, 0.4910],\n",
      "        [0.2120, 0.4698],\n",
      "        [0.1951, 0.5146],\n",
      "        [0.2557, 0.4992]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# checkä¸€ä¸‹\n",
    "outputs = model(**batch)\n",
    "print(outputs)\n",
    "# ç°åœ¨æœ‰äº†æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨ï¼Œè¿˜éœ€è¦ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc59ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr=5e-5) # å­¦ä¹ ç‡ä»æœ€å¤§å€¼ ï¼ˆ5e-5ï¼‰ åˆ° 0 çš„çº¿æ€§è¡°å‡\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs*len(train_dataloader)\n",
    "# å®šä¹‰çº¿æ€§å‡åˆ†çš„è°ƒåº¦å™¨ï¼Œï¼ˆ5e-5ï¼‰ åˆ° 0 æ˜¯å…¨éƒ¨å˜åŒ–ï¼Œæ€»å…±è¿­ä»£çš„æ•°æ®é‡*è¿­ä»£æ¬¡æ•° = æ€»å…±è®­ç»ƒäº†å¤šå°‘ä¸ªbatchï¼Œæ¯ä¸ªbatchçš„å­¦ä¹ ç‡å˜åŒ–é‡ = 5e-5/æ€»batchæ¬¡æ•°\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\"\n",
    "    ,optimizer=optimizer\n",
    "    ,num_warmup_steps=0\n",
    "    ,num_training_steps=num_training_steps\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d94f9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d687ca6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22849f9a2b54c659f818b499c4830ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# å¼€å§‹è®­ç»ƒ tqdmæ˜¯è¿›åº¦æ¡\n",
    "from tqdm.auto import tqdm\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "model.train() # æ¨¡å‹è®¾ç½®åˆ°è®­ç»ƒæ¨¡å¼\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # å¼ é‡è¦è½¬ç§»åˆ°gpuä¸Š\n",
    "        batch = {k:v.to(device) for k,v in batch.items()} #{labels:å¼ é‡æ•°æ®,intpus:å¼ é‡æ•°æ®}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss  # softmaxä»¥åŠcrossentropyçš„æŸå¤±å‡½æ•°å†…ç½®åœ¨äº†æ¨¡å‹å†…äº†\n",
    "        loss.backward() # æŸå¤±å‡½æ•°åå‘é€’å½’ï¼Œwå’Œbéƒ½ç§¯ç´¯äº†å˜åŠ¨å‚æ•°\n",
    "        \n",
    "        optimizer.step()  # ä¼˜åŒ–å™¨æ ¹æ®å˜åŠ¨å‚æ•°æ›´æ–° wï¼Œb\n",
    "        lr_scheduler.step() # è°ƒåº¦å™¨ä¿®æ”¹ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡å‚æ•°\n",
    "        optimizer.zero_grad() # æ›´æ–°å®Œäº†wå’Œbåæ¸…ç©ºç´¯ç§¯çš„å˜åŠ¨å‚æ•°\n",
    "        progress_bar.update(1) # ä¸€ä¸ªæ‰¹æ¬¡è®­ç»ƒå®Œäº†è¿›åº¦+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "642fc864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 1377)\n"
     ]
    }
   ],
   "source": [
    "print(range(num_training_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "413c9263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8578431372549019, 'f1': 0.9003436426116839}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¯„ä¼°\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\",\"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k:v.to(device) for k,v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits,dim=-1)\n",
    "    metric.add_batch(predictions=predictions,references=batch['labels'])\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1aa2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»¥ä¸Šä»£ç çš„å®Œå…¨ä½“\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_scheduler\n",
    "import evaluate\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "dataset = load_dataset(\"glue\",\"mrpc\")\n",
    "# data process\n",
    "def tokenize_func(input):\n",
    "    return tokenizer(input['sentence1'],input['sentence2'],truncation=True)\n",
    "dataset = dataset.map(tokenize_func)\n",
    "dataset = dataset.remove_columns(['sentence1','sentence2','idx'])\n",
    "dataset = dataset.rename_column('label','labels')\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    dataset['train']\n",
    "    ,batch_size=8\n",
    "    ,shuffle=True\n",
    "    ,collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    dataset['validation']\n",
    "    ,batch_size=8\n",
    "    ,shuffle=True\n",
    "    ,collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "# train\n",
    "model.train()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr=5e-5)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\"\n",
    "    ,optimizer=optimizer\n",
    "    ,num_warmup_steps=0\n",
    "    ,num_training_steps=num_epochs*len(train_dataloader)\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "process_bar = tqdm(range(num_epochs*len(train_dataloader)))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        outputs.loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        process_bar.update(1)\n",
    "\n",
    "# evaluate\n",
    "metric = evaluate.load(\"glue\",\"mrpc\")\n",
    "model.eval()\n",
    "for batch in valid_dataloader:\n",
    "    batch = {k:v.to(device) for k,v in batch.items()}\n",
    "    predictions = torch.argmax(outputs.logits,dim=-1)\n",
    "    metric.add_batch(predictions=predictions,references=batch['labels'])\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419d8b0",
   "metadata": {},
   "source": [
    "# AccelerateåŠ é€Ÿè®­ç»ƒï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6eae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'mrpc' at C:\\Users\\tassa\\.cache\\huggingface\\datasets\\glue\\mrpc\\0.0.0\\bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Thu May 22 15:04:29 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568f76846fec4ff997f472908cde450d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(),3e-5)\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model.to(device)\n",
    "train_dataloader,eval_dataloader,model,optimizer = accelerator.prepare(\n",
    "    train_dataloader,eval_dataloader,model,optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
